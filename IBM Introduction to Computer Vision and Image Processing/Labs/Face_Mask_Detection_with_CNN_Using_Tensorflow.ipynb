{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object (Face-Mask) Detection with Convolution Neural Network (CNN) based on Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZSL793i7KuM"
   },
   "source": [
    "## Import Libraries and Define Auxiliary Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries for OS and Cloud:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-deps lvis\n",
    "!pip install tf_slim\n",
    "!pip install --no-deps tensorflowjs==1.4.0\n",
    "!pip install tensorflow==1.15.2\n",
    "!pip install tensorflow_hub\n",
    "\n",
    "from IPython.display import display, Javascript, Image\n",
    "import re\n",
    "import zipfile\n",
    "from base64 import b64decode\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "import io\n",
    "import random\n",
    "\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "import six.moves.urllib as urllib\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "\n",
    "from skillsnetwork import cvstudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries for Tensorflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from object_detection.utils import dataset_util, label_map_util, config_util\n",
    "from object_detection.utils.label_map_util import get_label_map_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the files needed to build the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "if os.path.exists(\"content-latest.zip\"):\n",
    "    pass\n",
    "else:\n",
    "    !wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/CV0101/content/data/content-latest.zip\n",
    "    !wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/CV0101/content/data/tfrecord.py\n",
    "    \n",
    "with zipfile.ZipFile('content-latest.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:02<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CV Studio Client\n",
    "cvstudioClient = cvstudio.CVStudio()\n",
    "\n",
    "# Download All Images\n",
    "cvstudioClient.downloadAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the annotations from CV Studio\n",
    "annotations = cvstudioClient.get_annotations()\n",
    "labels = annotations['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join(os.getcwd(),'content/checkpoint')\n",
    "OUTPUT_PATH = os.path.join(os.getcwd(),'content/output')\n",
    "EXPORTED_PATH = os.path.join(os.getcwd(),'content/exported')\n",
    "DATA_PATH = os.path.join(os.getcwd(),'content/data')\n",
    "CONFIG_PATH = os.path.join(os.getcwd(),'content/config')\n",
    "LABEL_MAP_PATH = os.path.join(DATA_PATH, 'label_map.pbtxt')\n",
    "TRAIN_RECORD_PATH = os.path.join(DATA_PATH, 'train.record')\n",
    "VAL_RECORD_PATH = os.path.join(DATA_PATH, 'val.record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pre-Processing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "with open(LABEL_MAP_PATH, 'w') as f:\n",
    "    for idx, label in enumerate(labels):\n",
    "        f.write('item {\\n')\n",
    "        f.write(\"\\tname: '{}'\\n\".format(label))\n",
    "        f.write('\\tid: {}\\n'.format(idx + 1))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating TFRecord is Tensorflow’s binary storage format. Using a binary file format for storage of your data can have a significant impact on the performance of your import pipeline  as a consequence on the training time of your model. Go to this link https://www.tensorflow.org/tutorials/load_data/tfrecord to learn more about it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /resources/labs/cvstudio/object-detection-using-pre-trained-models/runs/training-object-detection/tfrecord.py:10: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tfrecord import create_tf_record, displaydetectedobject\n",
    "image_files = [image for image in annotations[\"annotations\"].keys()]\n",
    "\n",
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_files)\n",
    "num_train = int(0.7 * len(image_files))\n",
    "train_examples = image_files[:num_train]\n",
    "val_examples = image_files[num_train:]\n",
    "\n",
    "create_tf_record(train_examples, annotations[\"annotations\"], label_map, os.path.join(os.getcwd(),'images'), TRAIN_RECORD_PATH)\n",
    "create_tf_record(val_examples, annotations[\"annotations\"], label_map, os.path.join(os.getcwd(),'images'), VAL_RECORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oHD1Jm0v7jfz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18'\n",
    "CONFIG_TYPE = 'ssd_mobilenet_v1_quantized_300x300_coco14_sync'\n",
    "download_base = 'http://download.tensorflow.org/models/object_detection/'\n",
    "model = MODEL_TYPE + '.tar.gz'\n",
    "tmp = '/resources/checkpoint.tar.gz'\n",
    "\n",
    "if not (os.path.exists(CHECKPOINT_PATH)):\n",
    "    # Download the checkpoint\n",
    "    opener = urllib.request.URLopener()\n",
    "    opener.retrieve(download_base + model, tmp)\n",
    "\n",
    "    # Extract all the `model.ckpt` files.\n",
    "    with tarfile.open(tmp) as tar:\n",
    "        for member in tar.getmembers():\n",
    "            member.name = os.path.basename(member.name)\n",
    "            if 'model.ckpt' in member.name:\n",
    "                tar.extract(member, path=CHECKPOINT_PATH)\n",
    "            if 'pipeline.config' in member.name:\n",
    "                tar.extract(member, path=CONFIG_PATH)\n",
    "\n",
    "    os.remove(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model Training Pipeline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8CVExv6HsJS",
    "outputId": "0ad0122c-7db8-4543-9be1-f7109fad0a31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/object_detection/utils/config_util.py:482: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Maybe overwriting model.ssd.num_classes: 2\n",
      "INFO:tensorflow:Maybe overwriting train_config.batch_size: 6\n",
      "INFO:tensorflow:Maybe overwriting train_input_path: /resources/labs/cvstudio/object-detection-using-pre-trained-models/runs/training-object-detection/content/data/train.record\n",
      "INFO:tensorflow:Maybe overwriting eval_input_path: /resources/labs/cvstudio/object-detection-using-pre-trained-models/runs/training-object-detection/content/data/val.record\n",
      "INFO:tensorflow:Maybe overwriting train_config.fine_tune_checkpoint: /resources/labs/cvstudio/object-detection-using-pre-trained-models/runs/training-object-detection/content/checkpoint/model.ckpt\n",
      "INFO:tensorflow:Maybe overwriting label_map_path: /resources/labs/cvstudio/object-detection-using-pre-trained-models/runs/training-object-detection/content/data/label_map.pbtxt\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/object_detection/utils/config_util.py:182: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "INFO:tensorflow:Writing pipeline config file to /resources/labs/cvstudio/object-detection-using-pre-trained-models/runs/training-object-detection/content/data/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "pipeline_skeleton = 'content/models/research/object_detection/samples/configs/' + CONFIG_TYPE + '.config'\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_skeleton)\n",
    "\n",
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "num_classes = len(label_map.keys())\n",
    "meta_arch = configs[\"model\"].WhichOneof(\"model\")\n",
    "\n",
    "override_dict = {\n",
    "  'model.{}.num_classes'.format(meta_arch): num_classes,\n",
    "  'train_config.batch_size': 6,\n",
    "  'train_input_path': TRAIN_RECORD_PATH,\n",
    "  'eval_input_path': VAL_RECORD_PATH,\n",
    "  'train_config.fine_tune_checkpoint': os.path.join(CHECKPOINT_PATH, 'model.ckpt'),\n",
    "  'label_map_path': LABEL_MAP_PATH\n",
    "}\n",
    "\n",
    "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict)\n",
    "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_config, DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNYIZK1xVNAa"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = [\n",
    "    f'home/jupyterlab/conda/envs/python/lib/python3.6',\n",
    "    f'content/models/research',\n",
    "    f'content/models/research/slim'\n",
    "]\n",
    "os.environ['PYTHONPATH'] = ':'.join(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "epochs = 40\n",
    "start_datetime = datetime.now()\n",
    "!python -m object_detection.model_main \\\n",
    "    --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "    --num_train_steps=$epochs \\\n",
    "    --num_eval_steps=100\n",
    "\n",
    "regex = re.compile(r\"model\\.ckpt-([0-9]+)\\.index\")\n",
    "numbers = [int(regex.search(f).group(1)) for f in os.listdir(OUTPUT_PATH) if regex.search(f)]\n",
    "TRAINED_CHECKPOINT_PREFIX = os.path.join(OUTPUT_PATH, 'model.ckpt-{}'.format(max(numbers)))\n",
    "\n",
    "!python3 -m object_detection.export_inference_graph \\\n",
    "  --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "  --trained_checkpoint_prefix=$TRAINED_CHECKPOINT_PREFIX \\\n",
    "  --output_directory=$EXPORTED_PATH\n",
    "end_datetime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Here you can specify your own image \n",
    "URL = 'https://cdn.cliqueinc.com/posts/289533/kamala-harris-face-mask-289533-1602269219518-square.700x0c.jpg' \n",
    "\n",
    "with urllib.request.urlopen(URL) as url:\n",
    "    with open('test.jpg', 'wb') as f:\n",
    "        f.write(url.read())\n",
    "image = Image.open('test.jpg')\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "n,img,accuracy=displaydetectedobject(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'epochs': epochs\n",
    "}\n",
    "\n",
    "result = cvstudioClient.report(started=start_datetime, completed=end_datetime, parameters=parameters, accuracy= round(float(accuracy),2)*100)\n",
    "\n",
    "if result.ok:\n",
    "    print('Congratulations your results have been reported back to CV Studio!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model for Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!tensorflowjs_converter \\\n",
    "  --input_format=tf_frozen_model \\\n",
    "  --output_format=tfjs_graph_model \\\n",
    "  --output_node_names='Postprocessor/ExpandDims_1,Postprocessor/Slice' \\\n",
    "  --quantization_bytes=1 \\\n",
    "  --skip_op_check \\\n",
    "  $EXPORTED_PATH/frozen_inference_graph.pb \\\n",
    "  .\n",
    "import json\n",
    "\n",
    "from object_detection.utils.label_map_util import get_label_map_dict\n",
    "\n",
    "label_map = get_label_map_dict(LABEL_MAP_PATH)\n",
    "label_array = [k for k in sorted(label_map, key=label_map.get)]\n",
    "\n",
    "with open(os.path.join('', 'labels.json'), 'w') as f:\n",
    "    json.dump(label_array, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cd model_web \n",
    "with ZipFile('model_web.zip','w') as zip:\n",
    "    zip.write('group1-shard1of2.bin')\n",
    "    zip.write('group1-shard2of2.bin')\n",
    "    zip.write('model.json')\n",
    "    zip.write('labels.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download All Images\n",
    "cvstudioClient.uploadModel('model_web.zip', {'epochs': epochs })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/originals/48/6b/bc/486bbc7b49a92e11070bca0e719f14e9.gif\" alt=\"W3Schools.com\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also deploy your model via Web Application or Web App. This allows users to interact with your model like a website. They can upload the image with a user interface and view the results. Let's see how we can deploy a web app in CV Studio. \n",
    "\n",
    "In CV Studio, go to the **Use Model** section and select **New Application**. Fill out the window as follows: give your model a name, select **Models in this project**, select **TEST-1-click Deploy your Model to Cloud (Code Engine)** and select the model from the training run as shown here:\n",
    "\n",
    "<p>\n",
    "<img  src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/Image_2021-05-20_at_8.19_AM.jpeg\"  alt=\"popup\" width=\"400\" height=\"500\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Once the window is filled out, press the **Create Application** button and your model will begin deploying.\n",
    "<p>\n",
    "<img  src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/C2FE713D-E007-40E6-9E8A-CDFF9C3EF8DA_4_5005_c.jpeg\"  alt=\"popup\" width=\"500\" height=\"100\">\n",
    "</p>\n",
    "\n",
    "Wait until the status changes from \"DEPLOYING\" to \"READY\". Once the status changes to \"READY\", your application is ready for you to use!\n",
    "\n",
    "<p>\n",
    "<img  src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/Image_2021-05-20_at_8.20_AM__1_.jpeg\"  alt=\"popup\" width=\"500\" height=\"100\">\n",
    "</p>\n",
    "\n",
    "You can press now press the URL to go to your web application!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nayef Abou Tayoun is a Cognitive Data Scientist in the Skill Network Group at IBM. Currently, Nayef is pursuing the Master of Management in Artificial Intelligence degree at Queen's University in Canada. Nayef focuses on Machine Learning, Deep Learning, and Computer Vision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2021-05-25  | 0.3  | Kathy  |  Modified Multiple Areas |\n",
    "| 2021-05-25  | 0.3  | Yasmine  |  Modified Multiple Areas |\n",
    "| 2020-07-20  | 0.2  | Azim  |  Modified Multiple Areas |\n",
    "| 2020-07-17  | 0.1  | Azim  |  Created Lab Template |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " object-detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
