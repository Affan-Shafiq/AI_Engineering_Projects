{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceenzSQcogbj"
      },
      "source": [
        "# Building Recurrent Neural Networks/Long-Short Term Memory Models Using TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8DVC-bSpogbw",
        "outputId": "b59edfa1-4b3e-4a2f-a8c4-22bdaaea4520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.9.1\n",
            "  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.1)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.1)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.11.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1)\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.1)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (24.1)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.1)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.1)\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1)\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.16.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.2)\n",
            "Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 24.3.25\n",
            "    Uninstalling flatbuffers-24.3.25:\n",
            "      Successfully uninstalled flatbuffers-24.3.25\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-aiplatform 1.70.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.26.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.26.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.15.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "pandas-gbq 0.23.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ed7510cf91104ee7906bd43f9fd3231f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow==2.9.1\n",
        "#!pip install numpy==1.21.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "id": "RzQByHLGogbz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "if not tf.__version__ == '2.9.1':\n",
        "    print(tf.__version__)\n",
        "    raise ValueError('please upgrade to TensorFlow 2.9.1, or restart your Kernel (Kernel->Restart & Clear Output)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BMQ6KyZBogb0"
      },
      "outputs": [],
      "source": [
        "#!mkdir data\n",
        "#!mkdir data/ptb\n",
        "#!wget -q -O data/ptb/reader.py https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week3/data/ptb/reader.py\n",
        "#!cp data/ptb/reader.py .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "_HvRsdvfogb1"
      },
      "outputs": [],
      "source": [
        "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "\"\"\"Utilities for parsing PTB text files.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def _read_words(filename):\n",
        "  with tf.io.gfile.GFile(filename, \"r\") as f:\n",
        "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
        "\n",
        "\n",
        "def _build_vocab(filename):\n",
        "  data = _read_words(filename)\n",
        "\n",
        "  counter = collections.Counter(data)\n",
        "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "  words, _ = list(zip(*count_pairs))\n",
        "  word_to_id = dict(zip(words, range(len(words))))\n",
        "\n",
        "  return word_to_id\n",
        "\n",
        "\n",
        "def _file_to_word_ids(filename, word_to_id):\n",
        "  data = _read_words(filename)\n",
        "  return [word_to_id[word] for word in data if word in word_to_id]\n",
        "\n",
        "\n",
        "def ptb_raw_data(data_path=None):\n",
        "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
        "\n",
        "  Reads PTB text files, converts strings to integer ids,\n",
        "  and performs mini-batching of the inputs.\n",
        "\n",
        "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
        "\n",
        "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "\n",
        "  Args:\n",
        "    data_path: string path to the directory where simple-examples.tgz has\n",
        "      been extracted.\n",
        "\n",
        "  Returns:\n",
        "    tuple (train_data, valid_data, test_data, vocabulary)\n",
        "    where each of the data objects can be passed to PTBIterator.\n",
        "  \"\"\"\n",
        "\n",
        "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
        "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
        "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
        "\n",
        "  word_to_id = _build_vocab(train_path)\n",
        "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
        "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
        "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
        "  vocabulary = len(word_to_id)\n",
        "  return train_data, valid_data, test_data, vocabulary, word_to_id\n",
        "\n",
        "\n",
        "def ptb_iterator(raw_data, batch_size, num_steps):\n",
        "  \"\"\"Iterate on the raw PTB data.\n",
        "\n",
        "  This generates batch_size pointers into the raw PTB data, and allows\n",
        "  minibatch iteration along these pointers.\n",
        "\n",
        "  Args:\n",
        "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
        "    batch_size: int, the batch size.\n",
        "    num_steps: int, the number of unrolls.\n",
        "\n",
        "  Yields:\n",
        "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
        "    The second element of the tuple is the same data time-shifted to the\n",
        "    right by one.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if batch_size or num_steps are too high.\n",
        "  \"\"\"\n",
        "  raw_data = np.array(raw_data, dtype=np.int32)\n",
        "\n",
        "  data_len = len(raw_data)\n",
        "  batch_len = data_len // batch_size\n",
        "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
        "  for i in range(batch_size):\n",
        "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
        "\n",
        "  epoch_size = (batch_len - 1) // num_steps\n",
        "\n",
        "  if epoch_size == 0:\n",
        "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
        "\n",
        "  for i in range(epoch_size):\n",
        "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
        "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
        "    yield (x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtUYsFgzogb2"
      },
      "source": [
        "### Building the LSTM model for Language Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obGH7z9Bogb2",
        "outputId": "48be5afd-0001-4234-dd79-9249149ae57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-15 14:47:37--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/SPqCgT4JZp9royRjGgbqSA/data.zip\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2020687 (1.9M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   1.93M  3.62MB/s    in 0.5s    \n",
            "\n",
            "2024-10-15 14:47:38 (3.62 MB/s) - ‘data.zip’ saved [2020687/2020687]\n",
            "\n",
            "Archive:  data.zip\n",
            "  inflating: data/ptb.test.txt       \n",
            "  inflating: data/ptb.train.txt      \n",
            "  inflating: data/ptb.valid.txt      \n"
          ]
        }
      ],
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/SPqCgT4JZp9royRjGgbqSA/data.zip\n",
        "!unzip -o data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "id": "9YKFiX7Togb3"
      },
      "outputs": [],
      "source": [
        "#Initial weight scale\n",
        "init_scale = 0.1\n",
        "#Initial learning rate\n",
        "learning_rate = 1.0\n",
        "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
        "max_grad_norm = 5\n",
        "#The number of layers in our model\n",
        "num_layers = 2\n",
        "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
        "num_steps = 20\n",
        "#The number of processing units (neurons) in the hidden layers\n",
        "hidden_size_l1 = 256\n",
        "hidden_size_l2 = 128\n",
        "#The maximum number of epochs trained with the initial learning rate\n",
        "max_epoch_decay_lr = 4\n",
        "#The total number of epochs in training\n",
        "max_epoch = 15\n",
        "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
        "#At 1, we ignore the Dropout Layer wrapping.\n",
        "keep_prob = 0.5\n",
        "#The decay for the learning rate\n",
        "decay = 0.5\n",
        "#The size for each batch of data\n",
        "batch_size = 30\n",
        "#The size of our vocabulary\n",
        "vocab_size = 10000\n",
        "embeding_vector_size= 200\n",
        "#Training flag to separate training from testing\n",
        "is_training = 1\n",
        "#Data directory for our dataset\n",
        "data_dir = \"data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "id": "lmqsYwCUogb5"
      },
      "outputs": [],
      "source": [
        "# Reads the data and separates it into training data, validation data and testing data\n",
        "raw_data = ptb_raw_data(data_dir)\n",
        "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtG6rrUyogb5",
        "outputId": "5e64d156-118b-440d-d4b1-0a4468b094d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "929589"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4UOZEyEogb6",
        "outputId": "745c4f54-6fec-4677-acb2-bc150411dca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
          ]
        }
      ],
      "source": [
        "def id_to_word(id_list):\n",
        "    line = []\n",
        "    for w in id_list:\n",
        "        for word, wid in word_to_id.items():\n",
        "            if wid == w:\n",
        "                line.append(word)\n",
        "    return line\n",
        "\n",
        "\n",
        "print(id_to_word(train_data[0:100]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [],
        "id": "NTmhJMNTogb7"
      },
      "outputs": [],
      "source": [
        "itera = ptb_iterator(train_data, batch_size, num_steps)\n",
        "first_touple = itera.__next__()\n",
        "_input_data = first_touple[0]\n",
        "_targets = first_touple[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCPwyRsOogb7",
        "outputId": "a3c6c079-9109-4f77-b5c8-c6dbcdc77b51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "_input_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yi5eWBcogb8",
        "outputId": "53c2a493-087a-4ca4-9968-52f4546f039a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guXa-Zkkogb8",
        "outputId": "548d158b-e11f-4e9b-d557-e13d01d49705"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
              "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
              "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
              "         123,    7,  514,    2,   63,   10,  514,    8,  605],\n",
              "       [   0, 1071,    4,    0,  185,   24,  368,   20,   31, 3109,  954,\n",
              "          12,    3,   21,    2, 2915,    2,   12,    3,   21]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "_input_data[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-v2Exk8ogb9",
        "outputId": "c01652de-2bac-4dad-a0e1-9de9528af175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim']\n"
          ]
        }
      ],
      "source": [
        "print(id_to_word(_input_data[0,:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": [],
        "id": "_Ex1RnTrogb_"
      },
      "outputs": [],
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(vocab_size, embeding_vector_size,batch_input_shape=(batch_size, num_steps),trainable=True,name=\"embedding_vocab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aK320_2ogb_",
        "outputId": "a174449a-6bb8-44fd-a425-c84b8045cac6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30, 20, 200), dtype=float32, numpy=\n",
              "array([[[ 0.04628572, -0.03097992,  0.00240773, ..., -0.04494647,\n",
              "         -0.00347878,  0.01393459],\n",
              "        [-0.00488645, -0.03126304, -0.01484302, ..., -0.03757387,\n",
              "         -0.01726284,  0.02120251],\n",
              "        [ 0.01877541, -0.04161077,  0.02181014, ..., -0.00261043,\n",
              "         -0.006056  ,  0.02337099],\n",
              "        ...,\n",
              "        [-0.02855209,  0.00399095,  0.03136278, ..., -0.02801545,\n",
              "         -0.01248659,  0.0456008 ],\n",
              "        [-0.0160727 , -0.02884952, -0.00086705, ..., -0.01025946,\n",
              "         -0.04318186,  0.04106566],\n",
              "        [-0.01729947,  0.01848404, -0.02169886, ...,  0.02589177,\n",
              "         -0.03545898,  0.03210132]],\n",
              "\n",
              "       [[-0.02545879,  0.00100525, -0.03875574, ..., -0.00841223,\n",
              "         -0.0037452 ,  0.03212852],\n",
              "        [-0.04887114,  0.03633438, -0.04794074, ..., -0.02668142,\n",
              "          0.03375185, -0.02624012],\n",
              "        [-0.0435494 ,  0.02437932,  0.03671456, ..., -0.04177038,\n",
              "         -0.04392853, -0.0342554 ],\n",
              "        ...,\n",
              "        [-0.0342683 , -0.02262363, -0.04727724, ...,  0.03197522,\n",
              "          0.04284156, -0.04960123],\n",
              "        [-0.02337266,  0.0290493 , -0.02360079, ...,  0.02742485,\n",
              "         -0.0449453 , -0.00570846],\n",
              "        [-0.01686039, -0.04740317, -0.02338597, ...,  0.00567683,\n",
              "         -0.00458409, -0.04998114]],\n",
              "\n",
              "       [[-0.0351413 ,  0.0082525 , -0.02333017, ...,  0.01796185,\n",
              "          0.00142447,  0.01965215],\n",
              "        [-0.01042927, -0.03009887,  0.01033507, ...,  0.00480429,\n",
              "         -0.04644532,  0.01848057],\n",
              "        [ 0.02689509,  0.04454067, -0.04676826, ..., -0.04029157,\n",
              "          0.03881903, -0.02262115],\n",
              "        ...,\n",
              "        [-0.00204418, -0.01496745, -0.03141797, ..., -0.01257641,\n",
              "         -0.02070186, -0.03981274],\n",
              "        [-0.02530471,  0.01242588, -0.0476902 , ..., -0.04974275,\n",
              "         -0.04779093, -0.04608064],\n",
              "        [-0.01576877,  0.025336  ,  0.00621068, ..., -0.00646174,\n",
              "          0.00259858, -0.03432022]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-0.00385464,  0.02498568,  0.03902593, ...,  0.03892093,\n",
              "          0.04835654, -0.04075385],\n",
              "        [-0.01208381, -0.03005758, -0.04795796, ...,  0.00782192,\n",
              "         -0.02063254,  0.02308406],\n",
              "        [-0.00674929, -0.0130733 , -0.04117018, ..., -0.02644682,\n",
              "          0.02821602,  0.02163413],\n",
              "        ...,\n",
              "        [ 0.01086076,  0.04807061, -0.03829659, ..., -0.00494247,\n",
              "          0.01336113,  0.01147031],\n",
              "        [ 0.0437558 ,  0.03154862, -0.04147515, ...,  0.03743539,\n",
              "         -0.01602788, -0.03766779],\n",
              "        [ 0.0200439 ,  0.01377029, -0.03415723, ...,  0.04777808,\n",
              "          0.01665409, -0.02781726]],\n",
              "\n",
              "       [[ 0.03159863, -0.00859759, -0.04313215, ...,  0.03363163,\n",
              "         -0.01399631,  0.0404118 ],\n",
              "        [ 0.00290478, -0.01735504, -0.027825  , ..., -0.01350141,\n",
              "          0.04126212, -0.03556746],\n",
              "        [ 0.02689509,  0.04454067, -0.04676826, ..., -0.04029157,\n",
              "          0.03881903, -0.02262115],\n",
              "        ...,\n",
              "        [ 0.00662291, -0.01995246,  0.04520433, ...,  0.00788604,\n",
              "         -0.02679701, -0.02565836],\n",
              "        [-0.04887114,  0.03633438, -0.04794074, ..., -0.02668142,\n",
              "          0.03375185, -0.02624012],\n",
              "        [-0.01243959, -0.02064184,  0.00456502, ..., -0.00247475,\n",
              "          0.0423257 ,  0.04585631]],\n",
              "\n",
              "       [[-0.00384334,  0.04663516, -0.039818  , ..., -0.02532457,\n",
              "          0.00994798,  0.00996836],\n",
              "        [-0.03066138,  0.0380376 ,  0.02295938, ..., -0.04566678,\n",
              "          0.01894468, -0.02268936],\n",
              "        [ 0.02340045,  0.01199559,  0.00746586, ...,  0.00357955,\n",
              "          0.00034461, -0.03195261],\n",
              "        ...,\n",
              "        [-0.03001993, -0.03686208,  0.03193584, ..., -0.04083698,\n",
              "         -0.01655574,  0.04561676],\n",
              "        [-0.00116153, -0.01959095,  0.02043556, ...,  0.04021208,\n",
              "          0.01858665,  0.03104774],\n",
              "        [-0.04519069,  0.03199301,  0.00245882, ...,  0.01786784,\n",
              "         -0.01155522,  0.00868971]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Define where to get the data for our embeddings from\n",
        "inputs = embedding_layer(_input_data)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k-NlHWeogcA"
      },
      "source": [
        "<h3>Constructing Recurrent Neural Networks</h3>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": [],
        "id": "ch3o35i8ogcA"
      },
      "outputs": [],
      "source": [
        "lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
        "lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": [],
        "id": "ToN5j7-AogcB"
      },
      "outputs": [],
      "source": [
        "stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": [],
        "id": "A-wznZyjogcC"
      },
      "outputs": [],
      "source": [
        "layer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": [],
        "id": "xxyhFn78ogcD"
      },
      "outputs": [],
      "source": [
        "init_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [],
        "id": "rmF9ErHOogcD"
      },
      "outputs": [],
      "source": [
        "layer.inital_state = init_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSqcv3mnogcE",
        "outputId": "c49648ca-0b8d-4cf2-f897-90a9c9c65d13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(30, 200) dtype=float32, numpy=\n",
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "layer.inital_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "tags": [],
        "id": "qSfnmK70ogcF"
      },
      "outputs": [],
      "source": [
        "outputs = layer(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt0F38beogcF",
        "outputId": "e86fd2e1-4dd5-4e1f-b3d0-3810d541081e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30, 20, 128), dtype=float32, numpy=\n",
              "array([[[ 9.12942982e-04, -8.01309769e-04, -3.02348140e-04, ...,\n",
              "          1.53320166e-03, -4.43523808e-04,  3.60070117e-05],\n",
              "        [ 4.07505053e-04, -1.39851146e-03, -1.69620709e-03, ...,\n",
              "          1.26759626e-03, -9.50086454e-04,  5.41495741e-04],\n",
              "        [-6.71674847e-04, -1.00509613e-03, -5.60034590e-04, ...,\n",
              "          3.58155259e-04,  1.65742007e-04,  1.17412338e-03],\n",
              "        ...,\n",
              "        [-1.47903233e-03,  2.03869655e-03,  2.13089329e-03, ...,\n",
              "         -3.33704217e-03,  2.92561878e-03, -3.34688113e-04],\n",
              "        [-1.99918728e-03,  1.04197999e-03,  1.87523046e-03, ...,\n",
              "         -2.89060501e-03,  1.95932784e-03,  1.93514279e-04],\n",
              "        [-1.79821381e-03,  9.36657772e-04,  2.50274246e-03, ...,\n",
              "         -3.15028057e-03,  1.11550861e-03,  5.76283128e-06]],\n",
              "\n",
              "       [[ 2.03793883e-04, -2.47581120e-05,  9.34942858e-04, ...,\n",
              "         -1.02057203e-03,  7.24201498e-04, -1.43109908e-04],\n",
              "        [-6.03578228e-04, -3.44451226e-04,  1.37796105e-05, ...,\n",
              "          8.84049769e-06,  1.62857154e-03,  1.68320583e-03],\n",
              "        [-2.04883469e-03, -8.85876885e-04, -6.87504478e-04, ...,\n",
              "          7.56818161e-04,  1.64234475e-03,  3.49799381e-03],\n",
              "        ...,\n",
              "        [-4.11385251e-03, -6.60840794e-03, -2.55307369e-03, ...,\n",
              "          1.08348783e-02,  6.15423452e-03,  5.55307663e-04],\n",
              "        [-4.87024570e-03, -7.44779734e-03, -3.89523408e-03, ...,\n",
              "          9.49368905e-03,  5.39979292e-03, -2.20126240e-04],\n",
              "        [-5.98330935e-03, -7.65863666e-03, -4.30168398e-03, ...,\n",
              "          7.92824198e-03,  4.68854234e-03,  1.45391459e-04]],\n",
              "\n",
              "       [[-9.40343423e-04,  2.41245129e-04, -1.08280847e-06, ...,\n",
              "         -3.64543579e-04, -3.32131982e-04,  4.17098490e-04],\n",
              "        [-1.26943795e-03,  8.01091723e-04,  7.32592962e-05, ...,\n",
              "         -3.46325047e-04, -1.57225784e-03,  4.69663064e-04],\n",
              "        [-2.60559004e-03,  2.60070967e-03,  2.51843565e-04, ...,\n",
              "         -1.37015828e-03, -3.06332926e-03,  2.23153620e-04],\n",
              "        ...,\n",
              "        [ 1.91836362e-03,  4.62267175e-03, -1.25684624e-03, ...,\n",
              "         -4.39525065e-05, -2.81939836e-04, -2.54618772e-03],\n",
              "        [ 2.84788897e-03,  4.12033871e-03, -2.98985490e-03, ...,\n",
              "         -8.33699960e-06, -1.00141042e-03, -1.83059671e-03],\n",
              "        [ 2.79869465e-03,  4.51380201e-03, -2.84892088e-03, ...,\n",
              "          2.39331872e-04, -9.99633223e-04, -1.62939227e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 7.94513209e-04,  1.82400923e-03, -1.15646784e-04, ...,\n",
              "         -5.18535206e-04,  8.66340270e-05,  3.71510541e-04],\n",
              "        [ 1.99536723e-03,  2.27134069e-03, -1.29954040e-03, ...,\n",
              "         -7.97922490e-04,  1.49739790e-03,  1.12555746e-03],\n",
              "        [ 2.39913445e-03,  2.09956383e-03, -1.95460743e-03, ...,\n",
              "         -5.90144045e-05,  2.96911574e-03,  3.18572111e-03],\n",
              "        ...,\n",
              "        [ 1.83861633e-03,  4.38147597e-03,  3.75091354e-03, ...,\n",
              "         -4.73653991e-03, -4.49501351e-03, -2.84063834e-04],\n",
              "        [ 1.60035968e-03,  5.82378777e-03,  2.73063499e-03, ...,\n",
              "         -4.24645096e-03, -4.03557578e-03, -8.75729660e-04],\n",
              "        [ 7.56776717e-04,  6.46229740e-03,  2.66487501e-03, ...,\n",
              "         -3.31244455e-03, -3.76620214e-03, -5.08366327e-04]],\n",
              "\n",
              "       [[-1.00115000e-03, -2.01327354e-03,  9.04764864e-04, ...,\n",
              "          4.30937973e-04, -1.81414618e-03,  7.07999279e-05],\n",
              "        [-2.11817981e-03, -3.53004341e-03,  1.45065971e-03, ...,\n",
              "          1.30299188e-03, -1.48535788e-03,  6.11483818e-04],\n",
              "        [-3.53261596e-03, -2.34721159e-03,  1.62249163e-03, ...,\n",
              "          3.75768228e-04, -1.51363516e-03,  5.74951177e-04],\n",
              "        ...,\n",
              "        [-5.31951711e-03, -4.99942107e-03,  8.71554390e-03, ...,\n",
              "         -4.02032747e-04,  3.57311569e-06,  2.29014724e-04],\n",
              "        [-7.48036895e-03, -5.41970227e-03,  7.84759503e-03, ...,\n",
              "          9.45968437e-04,  4.44149191e-04,  2.67525972e-03],\n",
              "        [-8.36709701e-03, -4.82845586e-03,  7.81477615e-03, ...,\n",
              "          8.87452392e-04,  1.27968239e-03,  4.38157143e-03]],\n",
              "\n",
              "       [[ 5.52877318e-04, -2.58864766e-05, -7.98018809e-05, ...,\n",
              "          1.98857888e-04,  1.85380384e-04, -5.38516208e-04],\n",
              "        [ 3.64307372e-04,  6.97784253e-06, -5.00344613e-04, ...,\n",
              "         -1.48418971e-04,  1.02599512e-03, -1.40421744e-03],\n",
              "        [ 2.46093550e-04, -7.31982698e-04, -8.02934926e-04, ...,\n",
              "          5.20151225e-04,  1.76529109e-03, -1.76303496e-03],\n",
              "        ...,\n",
              "        [-5.20469714e-03, -6.21513871e-04,  1.88683148e-03, ...,\n",
              "          8.60139204e-04, -5.61459805e-04,  6.11978211e-03],\n",
              "        [-6.24890672e-03, -3.36247671e-04,  3.13909096e-03, ...,\n",
              "          3.07357870e-04, -9.09152965e-04,  3.99467489e-03],\n",
              "        [-6.58435654e-03,  2.35038344e-04,  3.98832466e-03, ...,\n",
              "          7.15149159e-04, -1.91733323e-03,  1.72177271e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytSXM6KogcG"
      },
      "source": [
        "### Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "tags": [],
        "id": "KFcOfPX5ogcG"
      },
      "outputs": [],
      "source": [
        "dense = tf.keras.layers.Dense(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": [],
        "id": "3ha4ci_NogcH"
      },
      "outputs": [],
      "source": [
        "logits_outputs  = dense(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PfRPOFfogcH",
        "outputId": "2193714a-422e-4d7d-90ce-6d66d4354e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the output from dense layer:  (30, 20, 10000)\n"
          ]
        }
      ],
      "source": [
        "print(\"shape of the output from dense layer: \", logits_outputs.shape) #(batch_size, sequence_length, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbbF1swtogcI"
      },
      "source": [
        "### Activation layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "tags": [],
        "id": "1efq48b7ogcI"
      },
      "outputs": [],
      "source": [
        "activation = tf.keras.layers.Activation('softmax')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "tags": [],
        "id": "jqWaosjeogcJ"
      },
      "outputs": [],
      "source": [
        "output_words_prob = activation(logits_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK4JbTqkogcJ",
        "outputId": "064a2bed-a577-40b6-9885-9004784320f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the output from the activation layer:  (30, 20, 10000)\n"
          ]
        }
      ],
      "source": [
        "print(\"shape of the output from the activation layer: \", output_words_prob.shape) #(batch_size, sequence_length, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqVCL3c1ogcK",
        "outputId": "cf0d60fa-375c-494f-d192-084599273c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of observing words in t=0 to t=20 tf.Tensor(\n",
            "[[1.00013793e-04 1.00001926e-04 1.00013851e-04 ... 9.99997355e-05\n",
            "  1.00004239e-04 9.99829936e-05]\n",
            " [1.00028294e-04 9.99857220e-05 9.99879630e-05 ... 1.00000325e-04\n",
            "  9.99876138e-05 9.99746117e-05]\n",
            " [1.00023790e-04 9.99748299e-05 9.99730910e-05 ... 9.99939439e-05\n",
            "  9.99923941e-05 9.99824770e-05]\n",
            " ...\n",
            " [9.99906406e-05 9.99673139e-05 1.00175261e-04 ... 9.99688054e-05\n",
            "  1.00018944e-04 9.99508484e-05]\n",
            " [9.99947515e-05 9.99573676e-05 1.00171652e-04 ... 9.99627373e-05\n",
            "  1.00036174e-04 9.99592303e-05]\n",
            " [9.99816257e-05 9.99598560e-05 1.00141900e-04 ... 9.99457261e-05\n",
            "  1.00029501e-04 9.99705080e-05]], shape=(20, 10000), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0,0:num_steps])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ref2VsNiogcL"
      },
      "source": [
        "### Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfv7fApfogcL",
        "outputId": "8e45bb16-e2e5-4651-d25c-5c2ff0c5d633"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5356, 8774, 8774, 8566, 8566, 5861, 5861, 4266, 4266, 1523, 1523,\n",
              "       3101, 6178, 6178, 6178, 1069, 2408, 2408, 3840, 6087])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "np.argmax(output_words_prob[0,0:num_steps], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjQlKtFgogcM",
        "outputId": "93120316-1d30-4d28-9a0c-1bb4794a794f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
              "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "_targets[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "tags": [],
        "id": "tmq_SgumogcN"
      },
      "outputs": [],
      "source": [
        "def crossentropy(y_true, y_pred):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "tags": [],
        "id": "h3CY7rgZogcO"
      },
      "outputs": [],
      "source": [
        "loss  = crossentropy(_targets, output_words_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfY7LGX4ogcO",
        "outputId": "3a5f527a-5dec-4023-e780-4844dd48925b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
              "array([9.210342, 9.210597, 9.211056, 9.210915, 9.210085, 9.209992,\n",
              "       9.211426, 9.210394, 9.209596, 9.210715], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "loss[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXFET4hXogcP",
        "outputId": "98538c55-9c2d-475e-a69b-a89bc475f9e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=184.20763>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "cost = tf.reduce_sum(loss / batch_size)\n",
        "cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyECNviGogcQ"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3H2q6bUogcR",
        "outputId": "eb95a67e-1583-4f65-b5f5-8855e60400a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Create a variable for the learning rate\n",
        "lr = tf.Variable(0.0, trainable=False)\n",
        "optimizer = tf.keras.optimizers.SGD(lr=lr, clipnorm=max_grad_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBxLQyFWogcS",
        "outputId": "2b43d4d5-c09e-4ffd-d13a-37308acb3844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_vocab (Embedding)  (30, 20, 200)            2000000   \n",
            "                                                                 \n",
            " rnn (RNN)                   (30, 20, 128)             671088    \n",
            "                                                                 \n",
            " dense (Dense)               (30, 20, 10000)           1290000   \n",
            "                                                                 \n",
            " activation (Activation)     (30, 20, 10000)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,961,088\n",
            "Trainable params: 3,955,088\n",
            "Non-trainable params: 6,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(layer)\n",
        "model.add(dense)\n",
        "model.add(activation)\n",
        "model.compile(loss=crossentropy, optimizer=optimizer)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "tags": [],
        "id": "rhhrfi7uogcT"
      },
      "outputs": [],
      "source": [
        "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
        "tvars = model.trainable_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb--T5p7ogcU",
        "outputId": "7633155d-1f08-44ab-9944-339cc21c8244"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['embedding_vocab/embeddings:0',\n",
              " 'rnn/stacked_rnn_cells/lstm_cell/kernel:0',\n",
              " 'rnn/stacked_rnn_cells/lstm_cell/recurrent_kernel:0',\n",
              " 'rnn/stacked_rnn_cells/lstm_cell/bias:0',\n",
              " 'rnn/stacked_rnn_cells/lstm_cell_1/kernel:0',\n",
              " 'rnn/stacked_rnn_cells/lstm_cell_1/recurrent_kernel:0',\n",
              " 'rnn/stacked_rnn_cells/lstm_cell_1/bias:0',\n",
              " 'dense/kernel:0',\n",
              " 'dense/bias:0']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "[v.name for v in tvars]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "tags": [],
        "id": "7GTbAcfgogcW"
      },
      "outputs": [],
      "source": [
        "x = tf.constant(1.0)\n",
        "y =  tf.constant(2.0)\n",
        "with tf.GradientTape(persistent=True) as g:\n",
        "    g.watch(x)\n",
        "    g.watch(y)\n",
        "    func_test = 2 * x * x + 3 * x * y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE9gKKAcogcX",
        "outputId": "cdc95a5e-7639-4926-a4ad-849c86949c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(10.0, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "var_grad = g.gradient(func_test, x) # Will compute to 10.0\n",
        "print(var_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ5g3zXoogcX",
        "outputId": "45620d8a-b501-4c4d-8555-afe984a4df93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(3.0, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "var_grad = g.gradient(func_test, y) # Will compute to 3.0\n",
        "print(var_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "tags": [],
        "id": "XQuQXvMWogcY"
      },
      "outputs": [],
      "source": [
        "with tf.GradientTape() as tape:\n",
        "    # Forward pass.\n",
        "    output_words_prob = model(_input_data)\n",
        "    # Loss value for this batch.\n",
        "    loss  = crossentropy(_targets, output_words_prob)\n",
        "    cost = tf.reduce_sum(loss,axis=0) / batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "tags": [],
        "id": "stWqhluaogcZ"
      },
      "outputs": [],
      "source": [
        "# Get gradients of loss wrt the trainable variables.\n",
        "grad_t_list = tape.gradient(cost, tvars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C6KAlQeogcZ",
        "outputId": "fd1e01cc-ba12-4985-963e-aa310db75857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7d57bfffe050>, <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n",
            "array([[-1.3347782e-06, -3.4106438e-07,  4.5607811e-07, ...,\n",
            "        -2.4933527e-07, -5.7886632e-07, -3.0212448e-08],\n",
            "       [ 6.4784081e-07,  2.8341006e-08, -5.9881700e-08, ...,\n",
            "         3.3739576e-07, -1.9811230e-07, -8.3863711e-08],\n",
            "       [-6.3820505e-07,  5.0137305e-07,  3.8869780e-07, ...,\n",
            "        -3.5307789e-07, -1.0785769e-07, -2.5388249e-07],\n",
            "       ...,\n",
            "       [ 4.1377348e-07, -6.8695550e-07,  8.7615717e-07, ...,\n",
            "         1.2999340e-07, -2.4557602e-07, -3.5809654e-07],\n",
            "       [-1.1652573e-06,  4.4974945e-07,  1.1784881e-06, ...,\n",
            "         1.8469943e-07,  2.7766112e-07,  1.3589651e-07],\n",
            "       [-1.9000025e-07,  1.6800466e-07, -4.1431690e-07, ...,\n",
            "         1.9421735e-07,  5.0795364e-07, -1.6784658e-07]], dtype=float32)>, <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n",
            "array([[-2.16322050e-07, -2.02714602e-07, -4.52373683e-07, ...,\n",
            "         5.47986403e-08, -3.03866727e-08, -6.92402580e-09],\n",
            "       [ 7.31699927e-08, -3.48835215e-07, -5.62032199e-08, ...,\n",
            "         5.09691489e-08, -2.02000848e-07, -7.36810790e-09],\n",
            "       [ 1.35351605e-08,  1.32838352e-07, -1.02920765e-08, ...,\n",
            "         7.26162810e-08,  5.48854331e-08, -1.35930946e-08],\n",
            "       ...,\n",
            "       [-1.44838879e-07, -1.08190484e-07, -2.73761373e-07, ...,\n",
            "        -2.75336447e-08, -1.03489107e-07, -3.75534626e-09],\n",
            "       [-1.69703128e-07,  3.44041702e-08, -1.79553865e-07, ...,\n",
            "        -7.21370128e-08,  5.01486795e-07, -2.01966017e-08],\n",
            "       [-2.13219735e-07, -1.57921136e-07,  1.87943527e-08, ...,\n",
            "         1.10691431e-07, -1.11854504e-07,  4.35548088e-07]], dtype=float32)>, <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
            "array([3.8058995e-05, 1.2337186e-06, 2.7382654e-05, ..., 1.2657976e-05,\n",
            "       3.1490727e-06, 2.8041709e-06], dtype=float32)>, <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n",
            "array([[ 5.0934278e-07,  5.5011462e-08,  6.9154993e-09, ...,\n",
            "         8.6047734e-08, -8.6488278e-08, -1.6595871e-07],\n",
            "       [-1.5756110e-07,  1.8923427e-07,  2.8965547e-07, ...,\n",
            "         1.2173871e-07,  3.6619682e-08, -1.8587096e-07],\n",
            "       [ 5.1095157e-08, -4.3674113e-08,  2.1316781e-07, ...,\n",
            "         1.7467435e-08,  2.4746066e-07,  6.3697954e-08],\n",
            "       ...,\n",
            "       [-1.9232378e-07, -2.0735652e-07,  1.9697865e-07, ...,\n",
            "         7.6690576e-08,  8.3734335e-09, -1.3978230e-07],\n",
            "       [-1.8447690e-07,  1.8331357e-07, -7.5041555e-08, ...,\n",
            "        -1.4986851e-08, -1.9559579e-07, -1.8315990e-07],\n",
            "       [-1.7408226e-08, -1.8941347e-07,  8.4640916e-08, ...,\n",
            "         9.0162629e-08,  2.2748337e-07,  6.6524791e-08]], dtype=float32)>, <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n",
            "array([[ 4.0077492e-07, -4.5704567e-08, -2.1369434e-07, ...,\n",
            "        -9.4649778e-08, -1.3887629e-07,  1.2024519e-07],\n",
            "       [-2.7691645e-08, -1.2439065e-07,  9.2640761e-08, ...,\n",
            "         3.5683030e-08, -2.4241888e-08, -3.7509444e-09],\n",
            "       [-1.8737296e-07, -1.1290371e-08,  2.6802365e-07, ...,\n",
            "         5.8596548e-08,  1.2737547e-07, -1.2854498e-07],\n",
            "       ...,\n",
            "       [-1.4457639e-08,  8.2534264e-08, -1.9914835e-07, ...,\n",
            "        -1.1063093e-07,  2.7146932e-08,  1.1360765e-07],\n",
            "       [-8.5383434e-09,  7.6645854e-08, -2.0340394e-07, ...,\n",
            "        -6.3106015e-08, -3.6571626e-07,  1.5939969e-07],\n",
            "       [-6.7137059e-08, -1.0835290e-08, -2.0800209e-07, ...,\n",
            "        -1.4390166e-07, -1.3751604e-07,  2.2758022e-07]], dtype=float32)>, <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
            "array([-7.04536142e-05, -2.46596774e-05,  2.86111899e-05, -3.68277797e-05,\n",
            "        4.00073695e-05,  9.72453745e-06,  1.01522230e-04,  5.17130538e-05,\n",
            "        4.28444546e-05,  4.46292470e-05, -3.03142333e-05,  8.04733463e-06,\n",
            "       -2.99174644e-05,  4.76438654e-05,  6.08443543e-06, -4.86191129e-06,\n",
            "       -1.87399619e-05,  1.83984521e-05, -9.83756763e-05, -1.70253479e-05,\n",
            "        5.67878196e-05, -4.33765199e-05,  2.99208805e-05,  5.60685567e-06,\n",
            "       -1.86344587e-05,  1.03821330e-05, -5.50289224e-06, -9.50743561e-05,\n",
            "        4.99955058e-05, -4.06434738e-05, -2.34311774e-05, -4.24334321e-05,\n",
            "        1.69165723e-05,  2.03506243e-05,  1.79121307e-05,  7.33706620e-05,\n",
            "       -1.58311013e-05, -6.58724966e-05, -3.11374133e-05,  3.69968038e-05,\n",
            "        4.61809759e-05, -3.81699683e-05,  8.63215701e-06, -2.82178808e-05,\n",
            "       -3.02747212e-05,  1.43738725e-05, -4.63030083e-05,  2.21335904e-06,\n",
            "       -9.98450923e-06, -5.82309531e-06,  2.92999030e-05,  1.95559551e-07,\n",
            "       -1.06946736e-05, -3.38101345e-05,  4.36581686e-05,  7.95606538e-05,\n",
            "        4.28500534e-05, -4.02705118e-05,  4.61046111e-05,  3.15969883e-05,\n",
            "       -2.97533225e-05, -6.17406040e-05, -7.56442614e-06,  1.59260617e-05,\n",
            "       -7.63038861e-06,  3.59056503e-05,  4.45827500e-05,  3.03714078e-05,\n",
            "        8.23102309e-05,  2.53065045e-05,  2.82994788e-05,  1.14605544e-04,\n",
            "       -3.63086679e-06,  4.92607887e-06,  8.74490797e-05,  5.97425023e-05,\n",
            "        5.83666842e-05, -3.49192269e-05,  1.32435398e-05, -1.56877450e-05,\n",
            "       -3.79901612e-05, -5.38156382e-06, -4.38627212e-05, -2.60540037e-05,\n",
            "       -3.94542876e-05,  4.05337814e-05,  2.03216914e-05, -9.52119444e-06,\n",
            "       -1.33262083e-05,  1.02271133e-05, -2.83777681e-05,  9.06177811e-05,\n",
            "        4.46971262e-06,  3.22970809e-05,  6.84589177e-05, -2.38734128e-06,\n",
            "        1.36167728e-05, -3.67477624e-05, -4.04122256e-05, -1.46064209e-04,\n",
            "        4.08036758e-05,  2.70580495e-05, -4.01468242e-05, -4.77428785e-06,\n",
            "        6.84894039e-05,  5.92777433e-05, -1.44625537e-05,  3.74421834e-05,\n",
            "        4.91272149e-05,  4.56627786e-05, -8.56532279e-06,  4.51177475e-05,\n",
            "        3.87844775e-05,  3.28201750e-05,  4.43579620e-06,  2.54549614e-05,\n",
            "       -2.65518502e-05, -6.99608336e-06,  9.57376869e-06, -6.41802399e-06,\n",
            "        2.16963099e-06, -2.62733920e-05,  1.11601548e-06,  1.62398210e-05,\n",
            "       -4.72884349e-05,  1.08556396e-05,  2.28724093e-05, -6.38744314e-06,\n",
            "       -8.88487557e-05, -3.28868700e-05,  4.72050306e-05, -4.28461099e-05,\n",
            "       -5.52850315e-06,  5.31722253e-05,  1.32887013e-04,  8.11963109e-05,\n",
            "        7.37647933e-05,  6.24664899e-05, -5.49631113e-05, -9.75358671e-06,\n",
            "       -4.36120245e-05,  5.19588830e-05,  6.45161617e-06,  2.42412025e-05,\n",
            "       -5.38346867e-05,  2.28661265e-05, -1.16648240e-04, -8.67515882e-06,\n",
            "        8.69625728e-05, -8.91845120e-05,  6.52558520e-05, -1.26125196e-05,\n",
            "       -2.07368867e-05,  1.40718548e-05, -2.00186514e-05, -1.31562556e-04,\n",
            "        4.21238510e-05, -5.14563762e-05, -3.63960426e-05, -5.85077723e-05,\n",
            "       -7.72127351e-07,  1.57405993e-05,  2.29439520e-05,  8.44715687e-05,\n",
            "       -4.79648679e-06, -7.58490060e-05, -4.73422151e-05,  3.26896770e-05,\n",
            "        6.15206445e-05, -5.59921209e-05, -1.75978239e-05, -2.67657106e-05,\n",
            "       -4.23458550e-05, -4.51381857e-06, -5.61434063e-05,  4.47410457e-06,\n",
            "       -2.53672188e-05,  9.11489224e-07,  2.53960952e-05,  1.38451542e-05,\n",
            "       -6.58037607e-05, -4.27368250e-05,  5.18780034e-05,  8.90191877e-05,\n",
            "        4.68326361e-05, -8.93011602e-05,  8.48335694e-05,  4.37099370e-05,\n",
            "       -1.90477076e-05, -9.15904093e-05, -2.74803042e-05,  1.49974012e-05,\n",
            "       -3.13925884e-05,  6.42541927e-05,  6.65489933e-05,  3.76353346e-05,\n",
            "        1.50196065e-04,  4.64669174e-05,  4.33337882e-05,  1.60841329e-04,\n",
            "        3.56839155e-06,  1.63774821e-05,  7.99594345e-05,  7.83526775e-05,\n",
            "        7.67737074e-05, -5.20317335e-05, -3.76457365e-07, -3.98345583e-05,\n",
            "       -2.11441329e-05, -3.09713460e-06, -6.41781153e-05, -5.46776282e-05,\n",
            "       -5.80308333e-05,  5.23349227e-05, -1.36007402e-05,  9.79051720e-07,\n",
            "       -6.81362508e-06,  2.30638852e-05, -2.98305913e-05,  1.55072179e-04,\n",
            "       -2.52097452e-05,  4.80810268e-05,  8.94922632e-05,  1.40457323e-05,\n",
            "       -9.00705345e-07, -6.70009322e-05, -6.83109611e-05, -2.06624856e-04,\n",
            "        6.36670084e-05,  4.05218489e-05, -5.47780728e-05, -2.14705778e-06,\n",
            "        9.66288208e-05,  3.78562909e-05, -6.24601125e-06,  5.27389275e-05,\n",
            "        4.54527035e-05,  6.97847936e-05,  1.77219008e-05,  8.21650174e-05,\n",
            "        3.31055016e-05,  3.27961425e-05, -6.34925937e-06,  2.38093889e-05,\n",
            "       -4.49401996e-05,  3.13929013e-06,  3.65295637e-06, -3.38274003e-06,\n",
            "        2.72041743e-05, -2.73360711e-05, -2.72868942e-06,  1.69207451e-05,\n",
            "       -3.62209830e-05, -2.37519316e-06,  3.61130442e-05,  1.15489365e-06,\n",
            "        9.23715346e-03, -1.26147410e-02,  1.62948165e-02, -1.65371560e-02,\n",
            "       -4.69771447e-04,  3.55353020e-02,  1.54442955e-02, -6.28957078e-02,\n",
            "        9.87178832e-03, -1.78214330e-02,  4.70765978e-02, -2.12853439e-02,\n",
            "        6.13718107e-02,  3.53733338e-02,  3.26305814e-03, -6.77823275e-03,\n",
            "       -6.23802803e-02,  5.79198077e-03,  7.94241354e-02,  1.71933081e-02,\n",
            "       -1.48758842e-02,  3.62677984e-02,  6.00547716e-03, -1.35580096e-02,\n",
            "       -1.87078526e-03, -1.92273054e-02, -3.67104076e-03, -4.08293642e-02,\n",
            "       -4.30918634e-02, -2.26370208e-02,  2.56241933e-02, -6.38217153e-03,\n",
            "        8.74755718e-03,  2.20455825e-02, -5.67203294e-03, -2.43317895e-02,\n",
            "       -2.43477281e-02, -2.06009056e-02,  4.26951237e-02,  4.21700021e-03,\n",
            "        4.71340269e-02,  4.92336489e-02, -3.44861224e-02,  2.11021584e-02,\n",
            "        6.84534525e-03,  2.85092629e-02, -4.09423634e-02, -5.42778522e-02,\n",
            "        3.28328945e-02, -7.58625474e-03, -7.10716378e-03, -1.28407860e-02,\n",
            "       -3.39391008e-02, -2.26249136e-02, -1.09785739e-02,  7.55455494e-02,\n",
            "       -2.71423720e-04, -5.55725731e-02, -6.18484318e-02,  1.78387016e-02,\n",
            "        1.45527599e-02, -7.36965761e-02,  2.90828608e-02,  9.65798832e-03,\n",
            "        1.35923112e-02, -5.31610101e-02, -2.48701288e-03,  1.00385267e-02,\n",
            "        5.60741983e-02, -3.50101665e-02,  1.11852903e-02, -8.09205174e-02,\n",
            "       -1.71744917e-02,  7.30483979e-03,  1.59348957e-02, -4.29696888e-02,\n",
            "        3.76867168e-02,  3.77036333e-02,  2.28178455e-03, -4.19207886e-02,\n",
            "       -4.33084648e-03, -7.93325156e-03,  5.26174111e-03,  3.04530226e-02,\n",
            "       -2.90452112e-02,  3.99446785e-02, -3.68708968e-02, -5.09361364e-03,\n",
            "       -1.07485326e-02, -1.91247873e-02,  2.43588518e-02, -3.83483209e-02,\n",
            "        1.07791601e-03,  1.03679346e-02,  1.08509287e-02, -4.65634912e-02,\n",
            "        1.00441296e-02,  2.73724515e-02, -1.55845368e-02,  5.08064553e-02,\n",
            "        1.33924242e-02, -5.20032458e-03,  1.66182667e-02,  3.06061115e-02,\n",
            "       -4.82437424e-02, -3.09717585e-03, -2.20573992e-02, -2.96942554e-02,\n",
            "        5.14833592e-02, -4.04644758e-02, -2.08023302e-02, -3.81707773e-02,\n",
            "       -1.61801130e-02, -1.87286343e-02,  1.87881943e-02,  3.22035477e-02,\n",
            "        1.94064081e-02,  1.26825902e-03, -1.68995634e-02,  9.25198197e-03,\n",
            "       -1.56810228e-02,  1.53974919e-02,  3.20114270e-02,  1.75194908e-03,\n",
            "        6.69060796e-02, -9.01153497e-03, -3.48642655e-02,  2.92460732e-02,\n",
            "       -8.22033253e-05, -2.90436656e-05,  3.39406470e-05, -4.47076527e-05,\n",
            "        3.96072865e-05,  2.78308089e-05,  1.11768662e-04,  4.89763406e-05,\n",
            "        4.03712002e-05,  4.90350940e-05, -2.76917708e-05,  5.75155309e-06,\n",
            "       -3.80178972e-05,  5.48836833e-05,  1.40603024e-05,  4.21154709e-07,\n",
            "       -2.05567321e-05,  2.26800275e-05, -1.02528888e-04, -2.31566919e-05,\n",
            "        5.95440688e-05, -5.39406574e-05,  2.99664498e-05,  9.02084594e-06,\n",
            "       -2.01975290e-05,  5.50836376e-06, -1.16943802e-05, -1.05690655e-04,\n",
            "        4.79387818e-05, -4.01593570e-05, -3.12839729e-05, -4.69755287e-05,\n",
            "        1.03121838e-05,  1.49927437e-05,  1.91381860e-05,  8.04061419e-05,\n",
            "       -1.15414696e-05, -6.88012879e-05, -3.44353030e-05,  2.75360380e-05,\n",
            "        4.56635316e-05, -3.72341310e-05,  8.41399105e-06, -2.87800431e-05,\n",
            "       -3.90336572e-05,  1.39175800e-05, -5.64116126e-05,  3.50834489e-06,\n",
            "       -1.06347743e-05, -8.39358927e-06,  3.70526068e-05,  8.27447548e-06,\n",
            "       -2.27884120e-05, -2.70295823e-05,  4.79213631e-05,  8.59643260e-05,\n",
            "        3.85805397e-05, -5.12073893e-05,  4.78446382e-05,  4.59200855e-05,\n",
            "       -3.30687180e-05, -7.38581657e-05, -3.79142148e-06,  2.15170185e-05,\n",
            "       -1.30587505e-05,  4.44457546e-05,  5.12753468e-05,  2.40448662e-05,\n",
            "        9.12832620e-05,  3.15203906e-05,  3.75343006e-05,  1.20508084e-04,\n",
            "       -5.19566493e-06,  1.07741562e-05,  7.91069542e-05,  6.81532911e-05,\n",
            "        5.29762146e-05, -2.99992207e-05,  1.32677978e-05, -1.70863677e-05,\n",
            "       -4.99036614e-05,  3.75826130e-06, -4.77449976e-05, -3.66494860e-05,\n",
            "       -4.41671800e-05,  3.84678679e-05,  1.88206650e-05, -1.09744869e-05,\n",
            "       -4.25064945e-06,  1.25808883e-05, -3.27733214e-05,  1.07369691e-04,\n",
            "       -1.58612966e-06,  3.01149030e-05,  7.28833766e-05,  3.55266820e-06,\n",
            "        7.80062510e-06, -4.57896276e-05, -3.73148432e-05, -1.61411910e-04,\n",
            "        4.57872120e-05,  3.03971465e-05, -4.34058893e-05, -5.42570751e-06,\n",
            "        7.39039606e-05,  5.93262266e-05, -1.26970854e-05,  2.61223231e-05,\n",
            "        5.35213185e-05,  4.73021646e-05, -1.42166991e-05,  5.02448238e-05,\n",
            "        3.82645449e-05,  4.06675936e-05,  9.17683792e-06,  2.32826242e-05,\n",
            "       -3.40957886e-05, -1.49935204e-05,  1.15702314e-05,  1.71959755e-06,\n",
            "        1.33990725e-05, -2.88846531e-05, -9.86850227e-06,  9.52602204e-06,\n",
            "       -5.22241244e-05,  5.66869176e-06,  2.58334494e-05, -2.16089757e-06],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n",
            "array([[ 2.9078815e-03,  4.0453020e-03,  3.6757665e-03, ...,\n",
            "        -5.3757462e-06, -5.3716585e-06, -5.3752424e-06],\n",
            "       [-8.9595333e-04, -7.7829504e-04, -1.2288340e-03, ...,\n",
            "         2.4199298e-06,  2.4206654e-06,  2.4221863e-06],\n",
            "       [-1.9019013e-04, -2.0629070e-03, -1.0735993e-03, ...,\n",
            "         2.2066406e-06,  2.2072900e-06,  2.2068311e-06],\n",
            "       ...,\n",
            "       [ 3.0576793e-04, -6.3251355e-04,  6.7425543e-04, ...,\n",
            "        -8.6382204e-07, -8.6366481e-07, -8.6572078e-07],\n",
            "       [ 1.0416928e-03,  2.2040436e-03,  1.1105257e-03, ...,\n",
            "        -2.6556459e-06, -2.6541948e-06, -2.6556477e-06],\n",
            "       [ 1.6056819e-04, -2.9310459e-04, -5.5253739e-04, ...,\n",
            "         1.2412947e-06,  1.2396592e-06,  1.2408968e-06]], dtype=float32)>, <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
            "array([-0.7979981 , -1.0313313 , -1.0313321 , ...,  0.00200023,\n",
            "        0.00199957,  0.00200031], dtype=float32)>]\n"
          ]
        }
      ],
      "source": [
        "print(grad_t_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvSauX93ogca",
        "outputId": "82bc1e21-bddc-4f79-cf7c-1a0091f2f980"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x7d57c0303820>,\n",
              " <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n",
              " array([[-1.3347782e-06, -3.4106438e-07,  4.5607811e-07, ...,\n",
              "         -2.4933527e-07, -5.7886632e-07, -3.0212448e-08],\n",
              "        [ 6.4784081e-07,  2.8341006e-08, -5.9881700e-08, ...,\n",
              "          3.3739576e-07, -1.9811230e-07, -8.3863711e-08],\n",
              "        [-6.3820505e-07,  5.0137305e-07,  3.8869780e-07, ...,\n",
              "         -3.5307789e-07, -1.0785769e-07, -2.5388249e-07],\n",
              "        ...,\n",
              "        [ 4.1377348e-07, -6.8695550e-07,  8.7615717e-07, ...,\n",
              "          1.2999340e-07, -2.4557602e-07, -3.5809654e-07],\n",
              "        [-1.1652573e-06,  4.4974945e-07,  1.1784881e-06, ...,\n",
              "          1.8469943e-07,  2.7766112e-07,  1.3589651e-07],\n",
              "        [-1.9000025e-07,  1.6800466e-07, -4.1431690e-07, ...,\n",
              "          1.9421735e-07,  5.0795364e-07, -1.6784658e-07]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n",
              " array([[-2.16322050e-07, -2.02714602e-07, -4.52373683e-07, ...,\n",
              "          5.47986403e-08, -3.03866727e-08, -6.92402580e-09],\n",
              "        [ 7.31699927e-08, -3.48835215e-07, -5.62032199e-08, ...,\n",
              "          5.09691489e-08, -2.02000848e-07, -7.36810790e-09],\n",
              "        [ 1.35351605e-08,  1.32838352e-07, -1.02920765e-08, ...,\n",
              "          7.26162810e-08,  5.48854331e-08, -1.35930946e-08],\n",
              "        ...,\n",
              "        [-1.44838879e-07, -1.08190484e-07, -2.73761373e-07, ...,\n",
              "         -2.75336447e-08, -1.03489107e-07, -3.75534626e-09],\n",
              "        [-1.69703128e-07,  3.44041702e-08, -1.79553865e-07, ...,\n",
              "         -7.21370128e-08,  5.01486795e-07, -2.01966017e-08],\n",
              "        [-2.13219735e-07, -1.57921136e-07,  1.87943527e-08, ...,\n",
              "          1.10691431e-07, -1.11854504e-07,  4.35548088e-07]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
              " array([3.8058995e-05, 1.2337186e-06, 2.7382654e-05, ..., 1.2657976e-05,\n",
              "        3.1490727e-06, 2.8041709e-06], dtype=float32)>,\n",
              " <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n",
              " array([[ 5.0934278e-07,  5.5011462e-08,  6.9154993e-09, ...,\n",
              "          8.6047734e-08, -8.6488278e-08, -1.6595871e-07],\n",
              "        [-1.5756110e-07,  1.8923427e-07,  2.8965547e-07, ...,\n",
              "          1.2173871e-07,  3.6619682e-08, -1.8587096e-07],\n",
              "        [ 5.1095157e-08, -4.3674113e-08,  2.1316781e-07, ...,\n",
              "          1.7467435e-08,  2.4746066e-07,  6.3697954e-08],\n",
              "        ...,\n",
              "        [-1.9232378e-07, -2.0735652e-07,  1.9697865e-07, ...,\n",
              "          7.6690576e-08,  8.3734335e-09, -1.3978230e-07],\n",
              "        [-1.8447690e-07,  1.8331357e-07, -7.5041555e-08, ...,\n",
              "         -1.4986851e-08, -1.9559579e-07, -1.8315990e-07],\n",
              "        [-1.7408226e-08, -1.8941347e-07,  8.4640916e-08, ...,\n",
              "          9.0162629e-08,  2.2748337e-07,  6.6524791e-08]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n",
              " array([[ 4.0077492e-07, -4.5704567e-08, -2.1369434e-07, ...,\n",
              "         -9.4649778e-08, -1.3887629e-07,  1.2024519e-07],\n",
              "        [-2.7691645e-08, -1.2439065e-07,  9.2640761e-08, ...,\n",
              "          3.5683030e-08, -2.4241888e-08, -3.7509444e-09],\n",
              "        [-1.8737296e-07, -1.1290371e-08,  2.6802365e-07, ...,\n",
              "          5.8596548e-08,  1.2737547e-07, -1.2854498e-07],\n",
              "        ...,\n",
              "        [-1.4457639e-08,  8.2534264e-08, -1.9914835e-07, ...,\n",
              "         -1.1063093e-07,  2.7146932e-08,  1.1360765e-07],\n",
              "        [-8.5383434e-09,  7.6645854e-08, -2.0340394e-07, ...,\n",
              "         -6.3106015e-08, -3.6571626e-07,  1.5939969e-07],\n",
              "        [-6.7137059e-08, -1.0835290e-08, -2.0800209e-07, ...,\n",
              "         -1.4390166e-07, -1.3751604e-07,  2.2758022e-07]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
              " array([-7.04536142e-05, -2.46596774e-05,  2.86111899e-05, -3.68277797e-05,\n",
              "         4.00073695e-05,  9.72453745e-06,  1.01522230e-04,  5.17130538e-05,\n",
              "         4.28444546e-05,  4.46292470e-05, -3.03142333e-05,  8.04733463e-06,\n",
              "        -2.99174644e-05,  4.76438654e-05,  6.08443543e-06, -4.86191129e-06,\n",
              "        -1.87399619e-05,  1.83984521e-05, -9.83756763e-05, -1.70253479e-05,\n",
              "         5.67878196e-05, -4.33765199e-05,  2.99208805e-05,  5.60685567e-06,\n",
              "        -1.86344587e-05,  1.03821330e-05, -5.50289224e-06, -9.50743561e-05,\n",
              "         4.99955058e-05, -4.06434738e-05, -2.34311774e-05, -4.24334321e-05,\n",
              "         1.69165723e-05,  2.03506243e-05,  1.79121307e-05,  7.33706620e-05,\n",
              "        -1.58311013e-05, -6.58724966e-05, -3.11374133e-05,  3.69968038e-05,\n",
              "         4.61809759e-05, -3.81699683e-05,  8.63215701e-06, -2.82178808e-05,\n",
              "        -3.02747212e-05,  1.43738725e-05, -4.63030083e-05,  2.21335904e-06,\n",
              "        -9.98450923e-06, -5.82309531e-06,  2.92999030e-05,  1.95559551e-07,\n",
              "        -1.06946736e-05, -3.38101345e-05,  4.36581686e-05,  7.95606538e-05,\n",
              "         4.28500534e-05, -4.02705118e-05,  4.61046111e-05,  3.15969883e-05,\n",
              "        -2.97533225e-05, -6.17406040e-05, -7.56442614e-06,  1.59260617e-05,\n",
              "        -7.63038861e-06,  3.59056503e-05,  4.45827500e-05,  3.03714078e-05,\n",
              "         8.23102309e-05,  2.53065045e-05,  2.82994788e-05,  1.14605544e-04,\n",
              "        -3.63086679e-06,  4.92607887e-06,  8.74490797e-05,  5.97425023e-05,\n",
              "         5.83666842e-05, -3.49192269e-05,  1.32435398e-05, -1.56877450e-05,\n",
              "        -3.79901612e-05, -5.38156382e-06, -4.38627212e-05, -2.60540037e-05,\n",
              "        -3.94542876e-05,  4.05337814e-05,  2.03216914e-05, -9.52119444e-06,\n",
              "        -1.33262083e-05,  1.02271133e-05, -2.83777681e-05,  9.06177811e-05,\n",
              "         4.46971262e-06,  3.22970809e-05,  6.84589177e-05, -2.38734128e-06,\n",
              "         1.36167728e-05, -3.67477624e-05, -4.04122256e-05, -1.46064209e-04,\n",
              "         4.08036758e-05,  2.70580495e-05, -4.01468242e-05, -4.77428785e-06,\n",
              "         6.84894039e-05,  5.92777433e-05, -1.44625537e-05,  3.74421834e-05,\n",
              "         4.91272149e-05,  4.56627786e-05, -8.56532279e-06,  4.51177475e-05,\n",
              "         3.87844775e-05,  3.28201750e-05,  4.43579620e-06,  2.54549614e-05,\n",
              "        -2.65518502e-05, -6.99608336e-06,  9.57376869e-06, -6.41802399e-06,\n",
              "         2.16963099e-06, -2.62733920e-05,  1.11601548e-06,  1.62398210e-05,\n",
              "        -4.72884349e-05,  1.08556396e-05,  2.28724093e-05, -6.38744314e-06,\n",
              "        -8.88487557e-05, -3.28868700e-05,  4.72050306e-05, -4.28461099e-05,\n",
              "        -5.52850315e-06,  5.31722253e-05,  1.32887013e-04,  8.11963109e-05,\n",
              "         7.37647933e-05,  6.24664899e-05, -5.49631113e-05, -9.75358671e-06,\n",
              "        -4.36120245e-05,  5.19588830e-05,  6.45161617e-06,  2.42412025e-05,\n",
              "        -5.38346867e-05,  2.28661265e-05, -1.16648240e-04, -8.67515882e-06,\n",
              "         8.69625728e-05, -8.91845120e-05,  6.52558520e-05, -1.26125196e-05,\n",
              "        -2.07368867e-05,  1.40718548e-05, -2.00186514e-05, -1.31562556e-04,\n",
              "         4.21238510e-05, -5.14563762e-05, -3.63960426e-05, -5.85077723e-05,\n",
              "        -7.72127351e-07,  1.57405993e-05,  2.29439520e-05,  8.44715687e-05,\n",
              "        -4.79648679e-06, -7.58490060e-05, -4.73422151e-05,  3.26896770e-05,\n",
              "         6.15206445e-05, -5.59921209e-05, -1.75978239e-05, -2.67657106e-05,\n",
              "        -4.23458550e-05, -4.51381857e-06, -5.61434063e-05,  4.47410457e-06,\n",
              "        -2.53672188e-05,  9.11489224e-07,  2.53960952e-05,  1.38451542e-05,\n",
              "        -6.58037607e-05, -4.27368250e-05,  5.18780034e-05,  8.90191877e-05,\n",
              "         4.68326361e-05, -8.93011602e-05,  8.48335694e-05,  4.37099370e-05,\n",
              "        -1.90477076e-05, -9.15904093e-05, -2.74803042e-05,  1.49974012e-05,\n",
              "        -3.13925884e-05,  6.42541927e-05,  6.65489933e-05,  3.76353346e-05,\n",
              "         1.50196065e-04,  4.64669174e-05,  4.33337882e-05,  1.60841329e-04,\n",
              "         3.56839155e-06,  1.63774821e-05,  7.99594345e-05,  7.83526775e-05,\n",
              "         7.67737074e-05, -5.20317335e-05, -3.76457365e-07, -3.98345583e-05,\n",
              "        -2.11441329e-05, -3.09713460e-06, -6.41781153e-05, -5.46776282e-05,\n",
              "        -5.80308333e-05,  5.23349227e-05, -1.36007402e-05,  9.79051720e-07,\n",
              "        -6.81362508e-06,  2.30638852e-05, -2.98305913e-05,  1.55072179e-04,\n",
              "        -2.52097452e-05,  4.80810268e-05,  8.94922632e-05,  1.40457323e-05,\n",
              "        -9.00705345e-07, -6.70009322e-05, -6.83109611e-05, -2.06624856e-04,\n",
              "         6.36670084e-05,  4.05218489e-05, -5.47780728e-05, -2.14705778e-06,\n",
              "         9.66288208e-05,  3.78562909e-05, -6.24601125e-06,  5.27389275e-05,\n",
              "         4.54527035e-05,  6.97847936e-05,  1.77219008e-05,  8.21650174e-05,\n",
              "         3.31055016e-05,  3.27961425e-05, -6.34925937e-06,  2.38093889e-05,\n",
              "        -4.49401996e-05,  3.13929013e-06,  3.65295637e-06, -3.38274003e-06,\n",
              "         2.72041743e-05, -2.73360711e-05, -2.72868942e-06,  1.69207451e-05,\n",
              "        -3.62209830e-05, -2.37519316e-06,  3.61130442e-05,  1.15489365e-06,\n",
              "         9.23715346e-03, -1.26147410e-02,  1.62948165e-02, -1.65371560e-02,\n",
              "        -4.69771447e-04,  3.55353020e-02,  1.54442955e-02, -6.28957078e-02,\n",
              "         9.87178832e-03, -1.78214330e-02,  4.70765978e-02, -2.12853439e-02,\n",
              "         6.13718107e-02,  3.53733338e-02,  3.26305814e-03, -6.77823275e-03,\n",
              "        -6.23802803e-02,  5.79198077e-03,  7.94241354e-02,  1.71933081e-02,\n",
              "        -1.48758842e-02,  3.62677984e-02,  6.00547716e-03, -1.35580096e-02,\n",
              "        -1.87078526e-03, -1.92273054e-02, -3.67104076e-03, -4.08293642e-02,\n",
              "        -4.30918634e-02, -2.26370208e-02,  2.56241933e-02, -6.38217153e-03,\n",
              "         8.74755718e-03,  2.20455825e-02, -5.67203294e-03, -2.43317895e-02,\n",
              "        -2.43477281e-02, -2.06009056e-02,  4.26951237e-02,  4.21700021e-03,\n",
              "         4.71340269e-02,  4.92336489e-02, -3.44861224e-02,  2.11021584e-02,\n",
              "         6.84534525e-03,  2.85092629e-02, -4.09423634e-02, -5.42778522e-02,\n",
              "         3.28328945e-02, -7.58625474e-03, -7.10716378e-03, -1.28407860e-02,\n",
              "        -3.39391008e-02, -2.26249136e-02, -1.09785739e-02,  7.55455494e-02,\n",
              "        -2.71423720e-04, -5.55725731e-02, -6.18484318e-02,  1.78387016e-02,\n",
              "         1.45527599e-02, -7.36965761e-02,  2.90828608e-02,  9.65798832e-03,\n",
              "         1.35923112e-02, -5.31610101e-02, -2.48701288e-03,  1.00385267e-02,\n",
              "         5.60741983e-02, -3.50101665e-02,  1.11852903e-02, -8.09205174e-02,\n",
              "        -1.71744917e-02,  7.30483979e-03,  1.59348957e-02, -4.29696888e-02,\n",
              "         3.76867168e-02,  3.77036333e-02,  2.28178455e-03, -4.19207886e-02,\n",
              "        -4.33084648e-03, -7.93325156e-03,  5.26174111e-03,  3.04530226e-02,\n",
              "        -2.90452112e-02,  3.99446785e-02, -3.68708968e-02, -5.09361364e-03,\n",
              "        -1.07485326e-02, -1.91247873e-02,  2.43588518e-02, -3.83483209e-02,\n",
              "         1.07791601e-03,  1.03679346e-02,  1.08509287e-02, -4.65634912e-02,\n",
              "         1.00441296e-02,  2.73724515e-02, -1.55845368e-02,  5.08064553e-02,\n",
              "         1.33924242e-02, -5.20032458e-03,  1.66182667e-02,  3.06061115e-02,\n",
              "        -4.82437424e-02, -3.09717585e-03, -2.20573992e-02, -2.96942554e-02,\n",
              "         5.14833592e-02, -4.04644758e-02, -2.08023302e-02, -3.81707773e-02,\n",
              "        -1.61801130e-02, -1.87286343e-02,  1.87881943e-02,  3.22035477e-02,\n",
              "         1.94064081e-02,  1.26825902e-03, -1.68995634e-02,  9.25198197e-03,\n",
              "        -1.56810228e-02,  1.53974919e-02,  3.20114270e-02,  1.75194908e-03,\n",
              "         6.69060796e-02, -9.01153497e-03, -3.48642655e-02,  2.92460732e-02,\n",
              "        -8.22033253e-05, -2.90436656e-05,  3.39406470e-05, -4.47076527e-05,\n",
              "         3.96072865e-05,  2.78308089e-05,  1.11768662e-04,  4.89763406e-05,\n",
              "         4.03712002e-05,  4.90350940e-05, -2.76917708e-05,  5.75155309e-06,\n",
              "        -3.80178972e-05,  5.48836833e-05,  1.40603024e-05,  4.21154709e-07,\n",
              "        -2.05567321e-05,  2.26800275e-05, -1.02528888e-04, -2.31566919e-05,\n",
              "         5.95440688e-05, -5.39406574e-05,  2.99664498e-05,  9.02084594e-06,\n",
              "        -2.01975290e-05,  5.50836376e-06, -1.16943802e-05, -1.05690655e-04,\n",
              "         4.79387818e-05, -4.01593570e-05, -3.12839729e-05, -4.69755287e-05,\n",
              "         1.03121838e-05,  1.49927437e-05,  1.91381860e-05,  8.04061419e-05,\n",
              "        -1.15414696e-05, -6.88012879e-05, -3.44353030e-05,  2.75360380e-05,\n",
              "         4.56635316e-05, -3.72341310e-05,  8.41399105e-06, -2.87800431e-05,\n",
              "        -3.90336572e-05,  1.39175800e-05, -5.64116126e-05,  3.50834489e-06,\n",
              "        -1.06347743e-05, -8.39358927e-06,  3.70526068e-05,  8.27447548e-06,\n",
              "        -2.27884120e-05, -2.70295823e-05,  4.79213631e-05,  8.59643260e-05,\n",
              "         3.85805397e-05, -5.12073893e-05,  4.78446382e-05,  4.59200855e-05,\n",
              "        -3.30687180e-05, -7.38581657e-05, -3.79142148e-06,  2.15170185e-05,\n",
              "        -1.30587505e-05,  4.44457546e-05,  5.12753468e-05,  2.40448662e-05,\n",
              "         9.12832620e-05,  3.15203906e-05,  3.75343006e-05,  1.20508084e-04,\n",
              "        -5.19566493e-06,  1.07741562e-05,  7.91069542e-05,  6.81532911e-05,\n",
              "         5.29762146e-05, -2.99992207e-05,  1.32677978e-05, -1.70863677e-05,\n",
              "        -4.99036614e-05,  3.75826130e-06, -4.77449976e-05, -3.66494860e-05,\n",
              "        -4.41671800e-05,  3.84678679e-05,  1.88206650e-05, -1.09744869e-05,\n",
              "        -4.25064945e-06,  1.25808883e-05, -3.27733214e-05,  1.07369691e-04,\n",
              "        -1.58612966e-06,  3.01149030e-05,  7.28833766e-05,  3.55266820e-06,\n",
              "         7.80062510e-06, -4.57896276e-05, -3.73148432e-05, -1.61411910e-04,\n",
              "         4.57872120e-05,  3.03971465e-05, -4.34058893e-05, -5.42570751e-06,\n",
              "         7.39039606e-05,  5.93262266e-05, -1.26970854e-05,  2.61223231e-05,\n",
              "         5.35213185e-05,  4.73021646e-05, -1.42166991e-05,  5.02448238e-05,\n",
              "         3.82645449e-05,  4.06675936e-05,  9.17683792e-06,  2.32826242e-05,\n",
              "        -3.40957886e-05, -1.49935204e-05,  1.15702314e-05,  1.71959755e-06,\n",
              "         1.33990725e-05, -2.88846531e-05, -9.86850227e-06,  9.52602204e-06,\n",
              "        -5.22241244e-05,  5.66869176e-06,  2.58334494e-05, -2.16089757e-06],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n",
              " array([[ 2.9078815e-03,  4.0453020e-03,  3.6757665e-03, ...,\n",
              "         -5.3757462e-06, -5.3716585e-06, -5.3752424e-06],\n",
              "        [-8.9595333e-04, -7.7829504e-04, -1.2288340e-03, ...,\n",
              "          2.4199298e-06,  2.4206654e-06,  2.4221863e-06],\n",
              "        [-1.9019013e-04, -2.0629070e-03, -1.0735993e-03, ...,\n",
              "          2.2066406e-06,  2.2072900e-06,  2.2068311e-06],\n",
              "        ...,\n",
              "        [ 3.0576793e-04, -6.3251355e-04,  6.7425543e-04, ...,\n",
              "         -8.6382204e-07, -8.6366481e-07, -8.6572078e-07],\n",
              "        [ 1.0416928e-03,  2.2040436e-03,  1.1105257e-03, ...,\n",
              "         -2.6556459e-06, -2.6541948e-06, -2.6556477e-06],\n",
              "        [ 1.6056819e-04, -2.9310459e-04, -5.5253739e-04, ...,\n",
              "          1.2412947e-06,  1.2396592e-06,  1.2408968e-06]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
              " array([-0.7979981 , -1.0313313 , -1.0313321 , ...,  0.00200023,\n",
              "         0.00199957,  0.00200031], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# Define the gradient clipping threshold\n",
        "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
        "grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P_sNQD2ogca"
      },
      "source": [
        "<h4> 4.Apply the optimizer to the variables/gradients tuple. </h4>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "tags": [],
        "id": "2nhxOUhPogca"
      },
      "outputs": [],
      "source": [
        "# Create the training TensorFlow Operation through our optimizer\n",
        "train_op = optimizer.apply_gradients(zip(grads, tvars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZxqqZTwogcb"
      },
      "source": [
        "<a id=\"ltsm\"></a>\n",
        "<h2>LSTM</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "tags": [],
        "id": "qJbxHAIyogcb"
      },
      "outputs": [],
      "source": [
        "class PTBModel(object):\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        ######################################\n",
        "        # Setting parameters for ease of use #\n",
        "        ######################################\n",
        "        self.batch_size = batch_size\n",
        "        self.num_steps = num_steps\n",
        "        self.hidden_size_l1 = hidden_size_l1\n",
        "        self.hidden_size_l2 = hidden_size_l2\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embeding_vector_size = embeding_vector_size\n",
        "        # Create a variable for the learning rate\n",
        "        self._lr = 1.0\n",
        "\n",
        "        ###############################################################################\n",
        "        # Initializing the model using keras Sequential API  #\n",
        "        ###############################################################################\n",
        "\n",
        "        self._model = tf.keras.models.Sequential()\n",
        "\n",
        "        ####################################################################\n",
        "        # Creating the word embeddings layer and adding it to the sequence #\n",
        "        ####################################################################\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            # Create the embeddings for our input data. Size is hidden size.\n",
        "            self._embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embeding_vector_size,batch_input_shape=(self.batch_size, self.num_steps),trainable=True,name=\"embedding_vocab\")  #[10000x200]\n",
        "            self._model.add(self._embedding_layer)\n",
        "\n",
        "\n",
        "        ##########################################################################\n",
        "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
        "        ##########################################################################\n",
        "        # Create the LSTM Cells.\n",
        "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
        "        # The argument  of LSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n",
        "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
        "        lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
        "        lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)\n",
        "\n",
        "\n",
        "\n",
        "        # By taking in the LSTM cells as parameters, the StackedRNNCells function junctions the LSTM units to the RNN units.\n",
        "        # RNN cell composed sequentially of stacked simple cells.\n",
        "        stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "        # Creating the input structure for our RNN #\n",
        "        ############################################\n",
        "        # Input structure is 20x[30x200]\n",
        "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
        "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
        "        # Feeding a batch of b sentences to a RNN:\n",
        "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.\n",
        "        # In step 2,  second word of each of the b sentences is input in parallel.\n",
        "        # The parallelism is only for efficiency.\n",
        "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly.\n",
        "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel.\n",
        "\n",
        "        ########################################################################################################\n",
        "        # Instantiating our RNN model and setting stateful to True to feed forward the state to the next layer #\n",
        "        ########################################################################################################\n",
        "\n",
        "        self._RNNlayer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)\n",
        "\n",
        "        # Define the initial state, i.e., the model state for the very first data point\n",
        "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
        "        self._initial_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)\n",
        "        self._RNNlayer.inital_state = self._initial_state\n",
        "\n",
        "        ############################################\n",
        "        # Adding RNN layer to keras sequential API #\n",
        "        ############################################\n",
        "        self._model.add(self._RNNlayer)\n",
        "\n",
        "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l1,return_sequences=True,stateful=True))\n",
        "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l2,return_sequences=True))\n",
        "\n",
        "\n",
        "        ####################################################################################################\n",
        "        # Instantiating a Dense layer that connects the output to the vocab_size  and adding layer to model#\n",
        "        ####################################################################################################\n",
        "        self._dense = tf.keras.layers.Dense(self.vocab_size)\n",
        "        self._model.add(self._dense)\n",
        "\n",
        "\n",
        "        ####################################################################################################\n",
        "        # Adding softmax activation layer and deriving probability to each class and adding layer to model #\n",
        "        ####################################################################################################\n",
        "        self._activation = tf.keras.layers.Activation('softmax')\n",
        "        self._model.add(self._activation)\n",
        "\n",
        "        ##########################################################\n",
        "        # Instantiating the stochastic gradient decent optimizer #\n",
        "        ##########################################################\n",
        "        self._optimizer = tf.keras.optimizers.SGD(lr=self._lr, clipnorm=max_grad_norm)\n",
        "\n",
        "\n",
        "        ##############################################################################\n",
        "        # Compiling and summarizing the model stacked using the keras sequential API #\n",
        "        ##############################################################################\n",
        "        self._model.compile(loss=self.crossentropy, optimizer=self._optimizer)\n",
        "        self._model.summary()\n",
        "\n",
        "\n",
        "    def crossentropy(self,y_true, y_pred):\n",
        "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    def train_batch(self,_input_data,_targets):\n",
        "        #################################################\n",
        "        # Creating the Training Operation for our Model #\n",
        "        #################################################\n",
        "        # Create a variable for the learning rate\n",
        "        self._lr = tf.Variable(0.0, trainable=False)\n",
        "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
        "        tvars = self._model.trainable_variables\n",
        "        # Define the gradient clipping threshold\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass.\n",
        "            output_words_prob = self._model(_input_data)\n",
        "            # Loss value for this batch.\n",
        "            loss  = self.crossentropy(_targets, output_words_prob)\n",
        "            # average across batch and reduce sum\n",
        "            cost = tf.reduce_sum(loss/ self.batch_size)\n",
        "        # Get gradients of loss wrt the trainable variables.\n",
        "        grad_t_list = tape.gradient(cost, tvars)\n",
        "        # Define the gradient clipping threshold\n",
        "        grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
        "        # Create the training TensorFlow Operation through our optimizer\n",
        "        train_op = self._optimizer.apply_gradients(zip(grads, tvars))\n",
        "        return cost\n",
        "\n",
        "    def test_batch(self,_input_data,_targets):\n",
        "        #################################################\n",
        "        # Creating the Testing Operation for our Model #\n",
        "        #################################################\n",
        "        output_words_prob = self._model(_input_data)\n",
        "        loss  = self.crossentropy(_targets, output_words_prob)\n",
        "        # average across batch and reduce sum\n",
        "        cost = tf.reduce_sum(loss/ self.batch_size)\n",
        "\n",
        "        return cost\n",
        "    @classmethod\n",
        "    def instance(cls) :\n",
        "        return PTBModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "tags": [],
        "id": "Dn4Qi07hogcd"
      },
      "outputs": [],
      "source": [
        "\n",
        "########################################################################################################################\n",
        "# run_one_epoch takes as parameters  the model instance, the data to be fed, training or testing mode and verbose info #\n",
        "########################################################################################################################\n",
        "def run_one_epoch(m, data,is_training=True,verbose=False):\n",
        "\n",
        "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
        "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
        "    start_time = time.time()\n",
        "    costs = 0.\n",
        "    iters = 0\n",
        "\n",
        "    m._model.reset_states()\n",
        "\n",
        "    #For each step and data point\n",
        "    for step, (x, y) in enumerate(ptb_iterator(data, m.batch_size, m.num_steps)):\n",
        "\n",
        "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
        "        #y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
        "        if is_training :\n",
        "            loss=  m.train_batch(x, y)\n",
        "        else :\n",
        "            loss = m.test_batch(x, y)\n",
        "\n",
        "\n",
        "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
        "        costs += loss\n",
        "\n",
        "        #Add number of steps to iteration counter\n",
        "        iters += m.num_steps\n",
        "\n",
        "        if verbose and step % (epoch_size // 10) == 10:\n",
        "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
        "\n",
        "\n",
        "\n",
        "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
        "    return np.exp(costs / iters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "tags": [],
        "id": "z88TDZx5ogcd"
      },
      "outputs": [],
      "source": [
        "# Reads the data and separates it into training data, validation data and testing data\n",
        "raw_data = ptb_raw_data(data_dir)\n",
        "train_data, valid_data, test_data, _, _ = raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxqA4BZ7ogce",
        "outputId": "ad12264b-ae96-436e-aa44-88fa58cc4daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_vocab (Embedding)  (30, 20, 200)            2000000   \n",
            "                                                                 \n",
            " rnn_1 (RNN)                 (30, 20, 128)             671088    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (30, 20, 10000)           1290000   \n",
            "                                                                 \n",
            " activation_1 (Activation)   (30, 20, 10000)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,961,088\n",
            "Trainable params: 3,955,088\n",
            "Non-trainable params: 6,000\n",
            "_________________________________________________________________\n",
            "Epoch 1 : Learning rate: 1.000\n",
            "Itr 10 of 1549, perplexity: 4607.672 speed: 530 wps\n",
            "Itr 164 of 1549, perplexity: 1095.470 speed: 983 wps\n",
            "Itr 318 of 1549, perplexity: 843.252 speed: 1004 wps\n",
            "Itr 472 of 1549, perplexity: 701.279 speed: 964 wps\n",
            "Itr 626 of 1549, perplexity: 600.515 speed: 978 wps\n",
            "Itr 780 of 1549, perplexity: 534.628 speed: 989 wps\n",
            "Itr 934 of 1549, perplexity: 482.629 speed: 998 wps\n",
            "Itr 1088 of 1549, perplexity: 442.968 speed: 1001 wps\n",
            "Itr 1242 of 1549, perplexity: 412.177 speed: 983 wps\n",
            "Itr 1396 of 1549, perplexity: 383.744 speed: 978 wps\n",
            "Epoch 1 : Train Perplexity: 361.197\n",
            "Epoch 1 : Valid Perplexity: 213.549\n",
            "Epoch 2 : Learning rate: 1.000\n",
            "Itr 10 of 1549, perplexity: 236.528 speed: 1159 wps\n",
            "Itr 164 of 1549, perplexity: 210.763 speed: 1055 wps\n",
            "Itr 318 of 1549, perplexity: 201.584 speed: 1039 wps\n",
            "Itr 472 of 1549, perplexity: 193.334 speed: 1048 wps\n",
            "Itr 626 of 1549, perplexity: 184.683 speed: 1046 wps\n",
            "Itr 780 of 1549, perplexity: 180.976 speed: 1033 wps\n",
            "Itr 934 of 1549, perplexity: 176.883 speed: 1036 wps\n",
            "Itr 1088 of 1549, perplexity: 173.488 speed: 1037 wps\n",
            "Itr 1242 of 1549, perplexity: 171.040 speed: 1037 wps\n",
            "Itr 1396 of 1549, perplexity: 166.979 speed: 1037 wps\n",
            "Epoch 2 : Train Perplexity: 164.101\n",
            "Epoch 2 : Valid Perplexity: 163.219\n",
            "Epoch 3 : Learning rate: 1.000\n",
            "Itr 10 of 1549, perplexity: 165.476 speed: 1156 wps\n",
            "Itr 164 of 1549, perplexity: 146.451 speed: 1003 wps\n",
            "Itr 318 of 1549, perplexity: 142.969 speed: 1011 wps\n",
            "Itr 472 of 1549, perplexity: 138.700 speed: 1002 wps\n",
            "Itr 626 of 1549, perplexity: 133.703 speed: 1015 wps\n",
            "Itr 780 of 1549, perplexity: 132.551 speed: 1018 wps\n",
            "Itr 934 of 1549, perplexity: 130.876 speed: 1016 wps\n",
            "Itr 1088 of 1549, perplexity: 129.485 speed: 1020 wps\n",
            "Itr 1242 of 1549, perplexity: 128.637 speed: 1023 wps\n",
            "Itr 1396 of 1549, perplexity: 126.381 speed: 1023 wps\n",
            "Epoch 3 : Train Perplexity: 125.091\n",
            "Epoch 3 : Valid Perplexity: 146.318\n",
            "Epoch 4 : Learning rate: 1.000\n",
            "Itr 10 of 1549, perplexity: 130.969 speed: 1149 wps\n",
            "Itr 164 of 1549, perplexity: 119.893 speed: 1035 wps\n",
            "Itr 318 of 1549, perplexity: 117.875 speed: 1031 wps\n",
            "Itr 472 of 1549, perplexity: 114.748 speed: 1026 wps\n",
            "Itr 626 of 1549, perplexity: 110.913 speed: 1031 wps\n",
            "Itr 780 of 1549, perplexity: 110.420 speed: 1032 wps\n",
            "Itr 934 of 1549, perplexity: 109.305 speed: 1031 wps\n",
            "Itr 1088 of 1549, perplexity: 108.439 speed: 1034 wps\n",
            "Itr 1242 of 1549, perplexity: 108.073 speed: 1034 wps\n",
            "Itr 1396 of 1549, perplexity: 106.435 speed: 1036 wps\n",
            "Epoch 4 : Train Perplexity: 105.650\n",
            "Epoch 4 : Valid Perplexity: 138.818\n",
            "Epoch 5 : Learning rate: 1.000\n",
            "Itr 10 of 1549, perplexity: 113.424 speed: 1151 wps\n",
            "Itr 164 of 1549, perplexity: 104.528 speed: 1045 wps\n",
            "Itr 318 of 1549, perplexity: 103.187 speed: 1033 wps\n",
            "Itr 472 of 1549, perplexity: 100.443 speed: 1020 wps\n",
            "Itr 626 of 1549, perplexity: 97.288 speed: 1028 wps\n",
            "Itr 780 of 1549, perplexity: 97.065 speed: 1027 wps\n",
            "Itr 934 of 1549, perplexity: 96.324 speed: 1026 wps\n",
            "Itr 1088 of 1549, perplexity: 95.684 speed: 1027 wps\n",
            "Itr 1242 of 1549, perplexity: 95.533 speed: 1027 wps\n",
            "Itr 1396 of 1549, perplexity: 94.211 speed: 1026 wps\n",
            "Epoch 5 : Train Perplexity: 93.666\n",
            "Epoch 5 : Valid Perplexity: 134.851\n",
            "Epoch 6 : Learning rate: 0.500\n",
            "Itr 10 of 1549, perplexity: 99.476 speed: 1168 wps\n",
            "Itr 164 of 1549, perplexity: 91.130 speed: 1049 wps\n",
            "Itr 318 of 1549, perplexity: 88.650 speed: 1045 wps\n",
            "Itr 472 of 1549, perplexity: 85.409 speed: 1046 wps\n",
            "Itr 626 of 1549, perplexity: 81.734 speed: 1033 wps\n",
            "Itr 780 of 1549, perplexity: 80.972 speed: 1034 wps\n",
            "Itr 934 of 1549, perplexity: 79.738 speed: 1035 wps\n",
            "Itr 1088 of 1549, perplexity: 78.614 speed: 1032 wps\n",
            "Itr 1242 of 1549, perplexity: 77.903 speed: 1033 wps\n",
            "Itr 1396 of 1549, perplexity: 76.241 speed: 1033 wps\n",
            "Epoch 6 : Train Perplexity: 75.234\n",
            "Epoch 6 : Valid Perplexity: 125.718\n",
            "Epoch 7 : Learning rate: 0.250\n",
            "Itr 10 of 1549, perplexity: 83.546 speed: 1159 wps\n",
            "Itr 164 of 1549, perplexity: 78.112 speed: 1020 wps\n",
            "Itr 318 of 1549, perplexity: 76.289 speed: 1025 wps\n",
            "Itr 472 of 1549, perplexity: 73.421 speed: 1034 wps\n",
            "Itr 626 of 1549, perplexity: 70.174 speed: 1030 wps\n",
            "Itr 780 of 1549, perplexity: 69.480 speed: 1032 wps\n",
            "Itr 934 of 1549, perplexity: 68.358 speed: 1034 wps\n",
            "Itr 1088 of 1549, perplexity: 67.273 speed: 1034 wps\n",
            "Itr 1242 of 1549, perplexity: 66.512 speed: 1030 wps\n",
            "Itr 1396 of 1549, perplexity: 64.943 speed: 1030 wps\n",
            "Epoch 7 : Train Perplexity: 63.930\n",
            "Epoch 7 : Valid Perplexity: 123.931\n",
            "Epoch 8 : Learning rate: 0.125\n",
            "Itr 10 of 1549, perplexity: 75.016 speed: 894 wps\n",
            "Itr 164 of 1549, perplexity: 70.837 speed: 1014 wps\n",
            "Itr 318 of 1549, perplexity: 69.351 speed: 1025 wps\n",
            "Itr 472 of 1549, perplexity: 66.794 speed: 1024 wps\n",
            "Itr 626 of 1549, perplexity: 63.814 speed: 1022 wps\n",
            "Itr 780 of 1549, perplexity: 63.180 speed: 1027 wps\n",
            "Itr 934 of 1549, perplexity: 62.150 speed: 1027 wps\n",
            "Itr 1088 of 1549, perplexity: 61.126 speed: 1027 wps\n",
            "Itr 1242 of 1549, perplexity: 60.385 speed: 1028 wps\n",
            "Itr 1396 of 1549, perplexity: 58.901 speed: 1028 wps\n",
            "Epoch 8 : Train Perplexity: 57.928\n",
            "Epoch 8 : Valid Perplexity: 123.492\n",
            "Epoch 9 : Learning rate: 0.062\n",
            "Itr 10 of 1549, perplexity: 70.705 speed: 920 wps\n",
            "Itr 164 of 1549, perplexity: 67.070 speed: 1037 wps\n",
            "Itr 318 of 1549, perplexity: 65.742 speed: 1040 wps\n",
            "Itr 472 of 1549, perplexity: 63.343 speed: 1042 wps\n",
            "Itr 626 of 1549, perplexity: 60.508 speed: 1041 wps\n",
            "Itr 780 of 1549, perplexity: 59.897 speed: 1037 wps\n",
            "Itr 934 of 1549, perplexity: 58.927 speed: 1036 wps\n",
            "Itr 1088 of 1549, perplexity: 57.943 speed: 1037 wps\n",
            "Itr 1242 of 1549, perplexity: 57.214 speed: 1036 wps\n",
            "Itr 1396 of 1549, perplexity: 55.780 speed: 1037 wps\n",
            "Epoch 9 : Train Perplexity: 54.829\n",
            "Epoch 9 : Valid Perplexity: 123.313\n",
            "Epoch 10 : Learning rate: 0.031\n",
            "Itr 10 of 1549, perplexity: 68.431 speed: 1173 wps\n",
            "Itr 164 of 1549, perplexity: 65.047 speed: 1030 wps\n",
            "Itr 318 of 1549, perplexity: 63.804 speed: 1045 wps\n",
            "Itr 472 of 1549, perplexity: 61.500 speed: 1046 wps\n",
            "Itr 626 of 1549, perplexity: 58.744 speed: 1043 wps\n",
            "Itr 780 of 1549, perplexity: 58.149 speed: 1047 wps\n",
            "Itr 934 of 1549, perplexity: 57.221 speed: 1043 wps\n",
            "Itr 1088 of 1549, perplexity: 56.263 speed: 1040 wps\n",
            "Itr 1242 of 1549, perplexity: 55.539 speed: 1042 wps\n",
            "Itr 1396 of 1549, perplexity: 54.129 speed: 1041 wps\n",
            "Epoch 10 : Train Perplexity: 53.190\n",
            "Epoch 10 : Valid Perplexity: 123.066\n",
            "Epoch 11 : Learning rate: 0.016\n",
            "Itr 10 of 1549, perplexity: 67.321 speed: 1042 wps\n",
            "Itr 164 of 1549, perplexity: 63.945 speed: 1055 wps\n",
            "Itr 318 of 1549, perplexity: 62.738 speed: 1051 wps\n",
            "Itr 472 of 1549, perplexity: 60.493 speed: 1055 wps\n",
            "Itr 626 of 1549, perplexity: 57.778 speed: 1048 wps\n",
            "Itr 780 of 1549, perplexity: 57.192 speed: 1052 wps\n",
            "Itr 934 of 1549, perplexity: 56.299 speed: 1051 wps\n",
            "Itr 1088 of 1549, perplexity: 55.355 speed: 1047 wps\n",
            "Itr 1242 of 1549, perplexity: 54.632 speed: 1048 wps\n",
            "Itr 1396 of 1549, perplexity: 53.235 speed: 1046 wps\n",
            "Epoch 11 : Train Perplexity: 52.302\n",
            "Epoch 11 : Valid Perplexity: 122.837\n",
            "Epoch 12 : Learning rate: 0.008\n",
            "Itr 10 of 1549, perplexity: 66.786 speed: 924 wps\n",
            "Itr 164 of 1549, perplexity: 63.339 speed: 1033 wps\n",
            "Itr 318 of 1549, perplexity: 62.145 speed: 1032 wps\n",
            "Itr 472 of 1549, perplexity: 59.932 speed: 1026 wps\n",
            "Itr 626 of 1549, perplexity: 57.238 speed: 1033 wps\n",
            "Itr 780 of 1549, perplexity: 56.656 speed: 1033 wps\n",
            "Itr 934 of 1549, perplexity: 55.785 speed: 1033 wps\n",
            "Itr 1088 of 1549, perplexity: 54.849 speed: 1033 wps\n"
          ]
        }
      ],
      "source": [
        "# Instantiates the PTBModel class\n",
        "m=PTBModel.instance()\n",
        "K = tf.keras.backend\n",
        "for i in range(max_epoch):\n",
        "    # Define the decay for this epoch\n",
        "    lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
        "    dcr = learning_rate * lr_decay\n",
        "    m._lr = dcr\n",
        "    K.set_value(m._model.optimizer.learning_rate,m._lr)\n",
        "    print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, m._model.optimizer.learning_rate))\n",
        "    # Run the loop for this epoch in the training mode\n",
        "    train_perplexity = run_one_epoch(m, train_data,is_training=True,verbose=True)\n",
        "    print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
        "\n",
        "    # Run the loop for this epoch in the validation mode\n",
        "    valid_perplexity = run_one_epoch(m, valid_data,is_training=False,verbose=False)\n",
        "    print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
        "\n",
        "# Run the loop in the testing mode to see how effective was our training\n",
        "test_perplexity = run_one_epoch(m, test_data,is_training=False,verbose=False)\n",
        "print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "prev_pub_hash": "dc854cc2a3cd5e8454549d14810f586268aa35a4c1ca674849cd9bf9674931b2",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}