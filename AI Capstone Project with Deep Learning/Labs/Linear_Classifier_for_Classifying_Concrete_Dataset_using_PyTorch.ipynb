{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpbxrw87KfgE"
      },
      "source": [
        "# Linear Classifier for Concrete Dataset Using PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ2KT0hKKfgF"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "q0f9yRI6KfgF"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import skillsnetwork"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW0i7T8gKfgG"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "8f2a8c5ec3504ae083a32c31ff52aa96",
            "dd276438b0ea4b2896654d092ef9552e"
          ]
        },
        "id": "N-g0z5bFKfgH",
        "outputId": "d03ffb02-96dd-4769-80d9-2d2e58ae91b2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f2a8c5ec3504ae083a32c31ff52aa96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading concrete_crack_images_for_classification.zip:   0%|          | 0/245259777 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd276438b0ea4b2896654d092ef9552e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to '../../../data'\n"
          ]
        }
      ],
      "source": [
        "await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVCKrq3_KfgI"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "P0Ul8KwqKfgI"
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/content\"\n",
        "        positive=\"Positive\"\n",
        "        negative=\"Negative\"\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n",
        "        positive_files.sort()\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n",
        "        negative_files.sort()\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files\n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "\n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:10000] # Change to 30000 to use the full test dataset\n",
        "            self.Y=self.Y[0:10000] # Change to 30000 to use the full test dataset\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)\n",
        "\n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "        image=Image.open(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "\n",
        "\n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBAqHB7jKfgI"
      },
      "source": [
        "<h2 id=\"trasform_Data_object\">Transform Object and Dataset Object</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "fn0eGRYvKfgJ"
      },
      "outputs": [],
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "#transforms.ToTensor()\n",
        "#transforms.Normalize(mean, std)\n",
        "#transforms.Compose([])\n",
        "\n",
        "transform =transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hMwBPsA9KfgJ"
      },
      "outputs": [],
      "source": [
        "dataset_train=Dataset(transform=transform,train=True)\n",
        "dataset_val=Dataset(transform=transform,train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DJLysk9aKfgJ",
        "outputId": "373cccfd-48ca-479f-beaf-24b43533c540"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 227, 227])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_train[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4yGWxNWgKfgK",
        "outputId": "c16d8a02-80b6-4bc0-d504-b42e59d08b7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "154587"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "size_of_image=3*227*227\n",
        "size_of_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "aNr1_72NLJIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_HZCCtYtKfgL",
        "outputId": "68be1e78-22f5-4211-95c9-fb0ca705ea3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f19901dfdb0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "d6OQInBTKfgL"
      },
      "outputs": [],
      "source": [
        "class Softmax(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(Softmax,self).__init__()\n",
        "        self.linear = nn.Linear(in_size, out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vJplE5xAKfgL"
      },
      "outputs": [],
      "source": [
        "model = Softmax(size_of_image, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "dCjr0ZseKfgM"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Z9jfplgYKfgM"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Rbz5PdvKKfgM"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=dataset_train, batch_size=5)\n",
        "val_loader = DataLoader(dataset=dataset_val, batch_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Go1KHq4OKfgN"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "n_epochs = 5\n",
        "loss_list = []\n",
        "accuracy_list = []\n",
        "N_test = len(dataset_val)\n",
        "\n",
        "def train_model(n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "        for x, y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            z = model(x.view(-1, size_of_image))\n",
        "            loss = criterion(z, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        correct = 0\n",
        "        # perform a prediction on the validation data\n",
        "        for x_test, y_test in val_loader:\n",
        "            z = model(x_test.view(-1, size_of_image))\n",
        "            _, yhat = torch.max(z.data, 1)\n",
        "            correct += (yhat == y_test).sum().item()\n",
        "        accuracy = correct / N_test\n",
        "        loss_list.append(loss.data)\n",
        "        accuracy_list.append(accuracy)\n",
        "\n",
        "train_model(n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "O148P9KsKfgN",
        "outputId": "0c1bf675-dbc0-4add-f502-cd1dffb7caaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83.54\n"
          ]
        }
      ],
      "source": [
        "print(max(accuracy_list) * 100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "prev_pub_hash": "d76b47f27f3838cee5fcf531fe9ce8abc439f204aaa935161f67b638f12e7c04",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}